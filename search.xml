<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[go 微服务实践(docker swarm)]]></title>
    <url>%2F2018%2F08%2F09%2Fgo%2Fgo%20%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[go微服务实践(docker swarm 版本…) go 微服务实践(docker swarm) [toc] 编写 web 服务docker 化 http 服务编译服务123export GOOS=linuxgo build -o accountservice-linux-amd64export GOOS=darwin docker file123456#docke file 文件FROM iron/baseEXPOSE 6767ADD accountservice-linux-amd64 /ENTRYPOINT [&quot;./accountservice-linux-amd64&quot;] build 镜像1build -t shihp/accountservice micro-account-service/ 启动镜像123docker container run --rm -p 8000:6767 -it shihp/micro-account-servicehttp://127.0.0.1:8000/accounts/10000 docker swarm 环境创建 machine1docker-machine create go-micro 依赖12345boot2docker#下载地址https://github.com/boot2docker/boot2docker/releases/download/v18.05.0-ce/boot2docker.iso#本机位置mv boot2docker.iso /Users/shihuipeng/.docker/machine/cache/ docker-machine swarm init12345docker $(docker-machine config go-micro) swarm init --advertise-addr $(docker-machine ip go-micro)# docker swarm join --token SWMTKN-1-36hq2nob9yu6ftovlb5xso6u27zh73m8u7nr1cfcefeja1fsov-c6edidql4h1x5uwgn0lq8zzvq 192.168.99.100:2377# docker swarm join-token manager 进入容器1eval $(docker-machine env swarm-manager-1) 创建网络1docker network create --driver overlay micro_network 运行 rabbitmq 服务1./rabbitmqshell 运行 spring could 服务1./springcloud.sh 运行 web 服务1./copyall.sh 运行询价服务1docker service create --name=quotes-service --replicas=1 --network=micro_network eriklupander/quotes-service 访问验证123docker-machine ip swarm-manager-1192.168.99.100:6767/accounts/10000 服务监控visualizer123456docker service create \ --name=viz \ --publish=8080:8080/tcp \ --constraint=node.role==manager \ --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \ dockersamples/visualizer 访问:192.168.99.100:8080 dvizz123456docker service create \ --constraint node.role==manager \ --replicas 1 --name dvizz -p 6969:6969 \ --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \ --network my_network \ eriklupander/dvizz 访问:192.168.99.100:6969 服务发现与负载均衡docker 服务缩放1docker service scale accountservice=3 springcould config 集中配置依赖 rabbitmq123456#!/bin/bash# RabbitMQdocker service rm rabbitmqdocker build -t shihp/rabbitmq ./docker service create --name=rabbitmq --replicas=1 --network=my_network -p 1883:1883 -p 5672:5672 -p 15672:15672 shihp/rabbitmq 依赖 viper 动态配置spring-could-config123456789#!/bin/bash# Config Servercd support/config-server./gradlew buildcd ../..docker build -t shihp/configserver support/config-server/docker service rm configserverdocker service create --replicas 1 --name configserver -p 8888:8888 --network my_network --update-delay 10s --with-registry-auth --update-parallelism 1 shihp/configserver 验证1http://192.168.99.100:8888/accountservice/dev/master 刷新services1234567891011121314# docker service 服务监控dvizz# go web 服务accountservice# java 微服务quotes-service# spring could config 服务 依赖rabbitmq# spring could config 服务configserver]]></content>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建 thrift PHP 环境]]></title>
    <url>%2F2018%2F07%2F04%2F%E8%BF%90%E7%BB%B4%2F%E6%90%AD%E5%BB%BA%20thrift%20%20PHP%20%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[搭建 thrift PHP 环境 php 使用 thrift 链接 hbase 编译安装 thrift中文说明 编译 php 扩展地址 thrift-0.11.0/lib/php/src/ext/thrift_protocol 代码示例使用示例]]></content>
      <tags>
        <tag>thrift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建 hbase 单机环境]]></title>
    <url>%2F2018%2F07%2F04%2F%E8%BF%90%E7%BB%B4%2F%E6%90%AD%E5%BB%BA%20hbase%20%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[搭建 hbase 单机环境 hbase 单机测试搭建 中文指南 软件地址 其中 bin 为可执行软件包,解压之后即可使用 安装依赖 java , 配置 JAVA_HOME ,yum 安装的 java_home 地址在export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.171-8.b10.el7_5.x86_64 配置地址在conf/hbase-env.sh thrift server1./bin/hbase thrift start 常用命令1234567891011hbase(main):003:0&gt; create &apos;test&apos;, &apos;cf&apos;0 row(s) in 1.2200 secondshbase(main):003:0&gt; list &apos;table&apos;test1 row(s) in 0.0550 secondshbase(main):004:0&gt; put &apos;test&apos;, &apos;row1&apos;, &apos;cf:a&apos;, &apos;value1&apos;0 row(s) in 0.0560 secondshbase(main):005:0&gt; put &apos;test&apos;, &apos;row2&apos;, &apos;cf:b&apos;, &apos;value2&apos;0 row(s) in 0.0370 secondshbase(main):006:0&gt; put &apos;test&apos;, &apos;row3&apos;, &apos;cf:c&apos;, &apos;value3&apos;0 row(s) in 0.0450 seconds]]></content>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go 微服务实践]]></title>
    <url>%2F2018%2F06%2F25%2Fgo%2Fgo%20%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[go 微服务实践 按照 博文 使用 go 搭建微服务系统 编写 web 服务docker 化 http 服务编译服务123export GOOS=linuxgo build -o accountservice-linux-amd64export GOOS=darwin docker file123456#docke file 文件FROM iron/baseEXPOSE 6767ADD accountservice-linux-amd64 /ENTRYPOINT [&quot;./accountservice-linux-amd64&quot;] build 镜像1build -t shihp/accountservice micro-account-service/ 启动镜像123docker container run --rm -p 8000:6767 -it shihp/micro-account-servicehttp://127.0.0.1:8000/accounts/10000 docker swarm 环境创建 machine1docker-machine create swarm-manager-1 依赖12345boot2docker#下载地址https://github.com/boot2docker/boot2docker/releases/download/v18.05.0-ce/boot2docker.iso#本机位置mv boot2docker.iso /Users/shihuipeng/.docker/machine/cache/ 创建docker-machine12345docker $(docker-machine config swarm-manager-1) swarm init --advertise-addr $(docker-machine ip swarm-manager-1)# docker swarm join --token SWMTKN-1-36hq2nob9yu6ftovlb5xso6u27zh73m8u7nr1cfcefeja1fsov-c6edidql4h1x5uwgn0lq8zzvq 192.168.99.100:2377# docker swarm join-token manager 进入容器1eval $(docker-machine env swarm-manager-1) 创建网络1docker network create --driver overlay my_network 运行web服务1docker service create --name=accountservice --replicas=1 --network=my_network -p=6767:6767 shihp/accountservice 访问验证123docker-machine ip swarm-manager-1192.168.99.100:6767/accounts/10000 服务监控visualizer123456docker service create \ --name=viz \ --publish=8080:8080/tcp \ --constraint=node.role==manager \ --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \ dockersamples/visualizer 访问:192.168.99.100:8080 dvizz123456docker service create \ --constraint node.role==manager \ --replicas 1 --name dvizz -p 6969:6969 \ --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \ --network my_network \ eriklupander/dvizz 访问:192.168.99.100:6969 询价服务1docker service create --name=quotes-service --replicas=1 --network=my_network eriklupander/quotes-service 服务发现与负载均衡docker 服务缩放1docker service scale accountservice=3 springcould config 集中配置依赖 rabbitmq123456#!/bin/bash# RabbitMQdocker service rm rabbitmqdocker build -t shihp/rabbitmq ./docker service create --name=rabbitmq --replicas=1 --network=my_network -p 1883:1883 -p 5672:5672 -p 15672:15672 shihp/rabbitmq 依赖 viper 动态配置spring-could-config123456789#!/bin/bash# Config Servercd support/config-server./gradlew buildcd ../..docker build -t shihp/configserver support/config-server/docker service rm configserverdocker service create --replicas 1 --name configserver -p 8888:8888 --network my_network --update-delay 10s --with-registry-auth --update-parallelism 1 shihp/configserver 验证1http://192.168.99.100:8888/accountservice/dev/master 刷新services1234567891011121314# docker service 服务监控dvizz# go web 服务accountservice# java 微服务quotes-service# spring could config 服务 依赖rabbitmq# spring could config 服务configserver]]></content>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gogs 应用部署]]></title>
    <url>%2F2018%2F04%2F27%2Fgo%2Fgogs%20%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一切以官网流程为主,本文档仅供一些概念细节参考 gogs nginx部署 配置ssl模式123456789101112131415161718192021server &#123; listen 443; server_name domain; #填写绑定证书的域名 ssl on; ssl_certificate 1_domain_bundle.crt; ssl_certificate_key 2_domain.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #按照这个协议配置 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;#按照这个套件配置 ssl_prefer_server_ciphers on; location / &#123; proxy_pass http://127.0.0.1:3000/; &#125; &#125;server &#123; listen 80; server_name domain; #填写绑定证书的域名 rewrite ^(.*) https://$server_name$1 permanent; &#125; gogs/custom/conf/app.ini最终生成的配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354APP_NAME = GogsRUN_USER = rootRUN_MODE = prod[database]DB_TYPE = mysqlHOST = 127.0.0.1:3306NAME = gogsUSER = userPASSWD = PASSWDSSL_MODE = disablePATH = data/gogs.db[repository]ROOT = /data/code/gogs-repositories[server]DOMAIN = domainHTTP_PORT = 3000ROOT_URL = https://domain/DISABLE_SSH = falseSSH_PORT = 22START_SSH_SERVER = falseOFFLINE_MODE = false[mailer]ENABLED = trueHOST = smtp.qq.com:465FROM = youmail.comUSER = youmailPASSWD = password[service]REGISTER_EMAIL_CONFIRM = trueENABLE_NOTIFY_MAIL = trueDISABLE_REGISTRATION = falseENABLE_CAPTCHA = trueREQUIRE_SIGNIN_VIEW = true[picture]DISABLE_GRAVATAR = falseENABLE_FEDERATED_AVATAR = false[session]PROVIDER = file[log]MODE = fileLEVEL = InfoROOT_PATH = /data/code/gogs-log[security]INSTALL_LOCK = trueSECRET_KEY = KkDhwmNuazMIsr1 一些细节关于运行用户选择官方推荐使用一个新的 gogs 用户来创建,我是使用 go 源码安装的,这块新创建用户,配置运行权限,之类的感觉很麻烦然后,我使用了日常的服务器账号(shihp)来作为运行的账户, 结果导致安装之后,我 ssh 登录不上…后来使用 root 来作为运行账户,暂时没遇到其他问题 关于 mysql8的问题这个已经有很多帖子了,赘述下vim /etc/my.cnf 追加1default-authentication-plugin = mysql_native_password 应该最好的方案还是升级/配置安全插件,不过有点麻烦,就算了. 有其他问题欢迎留言]]></content>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go 微服务实践 consul]]></title>
    <url>%2F2018%2F04%2F25%2Fgo%2Fgo%20%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%9E%E8%B7%B5%20consul%2F</url>
    <content type="text"><![CDATA[服务注册/发现 consul 1sudo yum install bind-utils 安装12345wget https://releases.hashicorp.com/consul/1.0.7/consul_1.0.7_freebsd_amd64.zipunzip consul_1.0.7_freebsd_amd64.zipcp consul /usr/local/bin 启动1consul agent -dev -node shihp 辅助工具123yum install bind-utilshistory | grep dig | grep -v grep | awk &apos;&#123;print $2 &quot; &quot; $3 &quot; &quot; $4 &quot; &quot; $5 &quot; &quot; $6 &quot; &quot; $7 &quot; &quot; $8&#125;&apos; 查看123curl localhost:8500/v1/catalog/nodesconsul membersdig @127.0.0.1 -p 8600 hello-world.node.consul 注册服务123456sudo mkdir /etc/consul.decho &apos;&#123;&quot;service&quot;: &#123;&quot;name&quot;: &quot;web&quot;, &quot;tags&quot;: [&quot;rails&quot;], &quot;port&quot;: 80&#125;&#125;&apos; \ | sudo tee /etc/consul.d/web.jsonconsul agent -dev -node shihp -config-dir=/etc/consul.d 检查服务 12345678dig @127.0.0.1 -p 8600 web.service.consuldig @127.0.0.1 -p 8600 web.service.consuldig @127.0.0.1 -p 8600 web.service.consul SRVdig @127.0.0.1 -p 8600 rails.web.service.consulcurl http://localhost:8500/v1/catalog/service/webcurl http://localhost:8500/v1/health/service/web?passing 创建集群Vagrantfilen1服务器one server123consul agent -server -bootstrap-expect=1 \ -data-dir=/tmp/consul -node=agent-one -bind=172.20.20.10 \ -enable-script-checks=true -config-dir=/etc/consul.d n2服务器one client.12consul agent -data-dir=/tmp/consul -node=agent-two \ -bind=172.20.20.11 -enable-script-checks=true -config-dir=/etc/consul.d 建立集群123vagrant ssh n1consul join 172.20.20.11consul members 自动集群建立Auto-joining a Cluster on Start (todo)12345678# Using a DNS entry$ consul agent -retry-join &quot;consul.domain.internal&quot;# Using IPv4$ consul agent -retry-join &quot;10.0.4.67&quot;# Using IPv6$ consul agent -retry-join &quot;[::1]:8301&quot;# Using Cloud Auto-Joining$ consul agent -retry-join &quot;provider=aws tag_key=...&quot; 集群健康检查 ping 检查123echo &apos;&#123;&quot;check&quot;: &#123;&quot;name&quot;: &quot;ping&quot;, &quot;args&quot;: [&quot;ping&quot;, &quot;-c1&quot;, &quot;google.com&quot;], &quot;interval&quot;: &quot;30s&quot;&#125;&#125;&apos; \ &gt;/etc/consul.d/ping.json web 检查123echo &apos;&#123;&quot;service&quot;: &#123;&quot;name&quot;: &quot;web&quot;, &quot;tags&quot;: [&quot;rails&quot;], &quot;port&quot;: 80, &quot;check&quot;: &#123;&quot;args&quot;: [&quot;curl&quot;, &quot;localhost&quot;], &quot;interval&quot;: &quot;10s&quot;&#125;&#125;&#125;&apos; \ &gt;/etc/consul.d/web.json 加载配置1consul reload 健康检查1curl http://localhost:8500/v1/health/state/critical web ui (todo) next]]></content>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beego 部署方案]]></title>
    <url>%2F2018%2F04%2F18%2Fgo%2Fbeego%20%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[因为在使用 beego 的时候,看官方文档,有些地方文档写的不够清楚,自己尝试了部署了下,然后将结果分享给大家,作为官方文档的辅助 前置工作 需要一个可以编译成功的 go 应用(简单点的话 可以直接 bee api {beeapi} 生成一份 api 的项目) 编译 go build 配置部署目录1234567mkdir /data/project/release/beego-hellocp beeapi /data/project/release/beego-hello/cp -fr conf /opt/app/beepkg//api 项目没有这两个文件夹,忽略即可cp -fr views /opt/app/beepkgcp -fr static /opt/app/beepkg 独立部署模式,将 go 应用启动1nohup ./beeapi &amp;&gt;&gt; beeapi.log &amp; 配置 nginx12345678910111213141516171819202122232425262728293031323334353637server &#123; listen 80; server_name beego.com; charset utf-8; access_log /data/project/release/beego-hello/beego.com.access.log; location / &#123; try_files /_not_exists_ @backend; &#125; location @backend &#123; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $http_host; proxy_pass http://127.0.0.1:8080; &#125;&#125;server &#123; listen 80; server_name beego-admin.com; charset utf-8; access_log /data/project/release/beego-hello/beego-admin.com.access.log; location / &#123; try_files /_not_exists_ @backend; &#125; location @backend &#123; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $http_host; proxy_pass http://127.0.0.1:8088; &#125;&#125; 验证配置域名映射1127.0.0.1 beego.com 压测1ab -n100000 -c200 http://beego.com/v1/object go 原生应用压测时候,,平均在120~170us 左右,没有 dump 过nginx,平均30~50us, 但是会 dump, 不稳定]]></content>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shadowsocks 下 在 终端 使用 privoxy Over the wall]]></title>
    <url>%2F2018%2F04%2F02%2Fefficient%2Fprivoxy%20over%20the%20wall%20in%20terminal%2F</url>
    <content type="text"><![CDATA[安装 privoxybrew install privoxy 配置 privoxy查看 代理使用的 ipvim ~/.ShadowsocksX/gfwlist.js 文件末尾添加配置vim /usr/local/etc/privoxy/config12listen-address 0.0.0.0:8118forward-socks5 / localhost:1080. 启用配置123sudo /usr/local/sbin/privoxy /usr/local/etc/privoxy/configexport http_proxy=&apos;http://localhost:8118&apos;export https_proxy=&apos;http://localhost:8118&apos; 检查curl ip.cn 设置快捷方式修改bashrcvim ~/.bashrc 配置别名12alias ss=&quot;export http_proxy=&apos;http://localhost:8118&apos; &amp;&amp; export https_proxy=&apos;http://localhost:8118&apos;&quot;alias ssd=&quot;unset http_proxy &amp;&amp; unset https_proxy&quot; 启用1source~/.bashrc 执行12ssssd]]></content>
      <tags>
        <tag>shadowsocks privoxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phalcon model save 使用]]></title>
    <url>%2F2018%2F02%2F23%2Fphp%2Fphalcon-model%20save%20%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[phalcon model 使用 save 报错 phalcon-model 使用 save方法时候,如果先进行查询,查询结果方法里面含有字段 source ,会替换掉目标表的名字 代码实例代码123$UserDemoteModel = self::findFirst([&quot;user_id=&#123;$userId&#125;&quot;]);$UserDemoteModel-&gt;level = 1;$UserDemoteModel-&gt;save(); 上述代码执行错误信息如下: phalcon Table &#39;1&#39; doesn&#39;t exist in database when dumping meta-data for Model\\*** 原因分析: phalcon-model 使用 save方法时候,如果先进行查询,查询结果方法里面含有字段 source ,会替换掉目标表的名字 调了有些时间,快速查询查询优化手段:1$this-&gt;getMessages(); 可以看到错误信息,不过不是很完整,类似 source is requier 或者 1$UserDemoteModel-&gt;getTargetTable(); 看到你目标表 为 1]]></content>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell 实现php 脚本动态扩展]]></title>
    <url>%2F2018%2F02%2F09%2F%E8%BF%90%E7%BB%B4%2Fshell%20%E5%AE%9E%E7%8E%B0php%20%E8%84%9A%E6%9C%AC%E5%8A%A8%E6%80%81%E6%89%A9%E5%B1%95%2F</url>
    <content type="text"><![CDATA[shell 通过 服务器可容纳的task数,以及总的需要执行的任务数 动态分配服务脚本 usage sh test.sh 100 1500000 123456789101112131415161718192021222324252627#!/bin/bash# shell 通过 服务器可容纳的task数,以及总的需要执行的任务数 动态分配服务脚本# task 参数 1: 后台启动的 task 数量# user_count 参数 2: 需要处理的总的任务量int=1task=$1user_count=$2task_deal_user=`expr $&#123;user_count&#125; / $&#123;task&#125;`do_task_deal_user=`expr $&#123;task_deal_user&#125; + 1`while(( $int&lt;=$&#123;task&#125; ))do start=`expr $&#123;int&#125; - 1` start2=`expr $&#123;start&#125; \* $&#123;do_task_deal_user&#125;` end=`expr $&#123;int&#125; \* $&#123;do_task_deal_user&#125;` echo &quot;$&#123;start2&#125; *** $&#123;start2&#125;&quot;# nohup php /data/project/vip/task/cli.php task main $&#123;start2&#125;,$&#123;end&#125; &quot;&gt;&quot; 100 &amp;&gt;/dev/null &amp; str=&quot;nohup php /data/project/vip/task/cli.php task main $start2,$end &gt; 100 &amp;&gt;/dev/null &amp;&quot; echo $&#123;str&#125; let &quot;int++&quot; sleep 1done]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phalcon 多数据源 model 封装]]></title>
    <url>%2F2018%2F02%2F01%2Fphp%2Fphalcon-model-implement%2F</url>
    <content type="text"><![CDATA[代码实现数据模型优化 优化数据连接服务注册单例 优化数据连接选择单例 优化多库原子级别轮训 优化 redis轮训 key配置0侵入,自动创建,自动扩容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203&lt;?phpnamespace Model;use Exception;use Lib\Logger\Adapter\Logger;use Phalcon\Db\Adapter\Pdo\Mysql;use Phalcon\DI;use Phalcon\Db;use Phalcon\Mvc\Model;use Lib\Extend\Cache\Redis;class BaseModel extends Model&#123; private $dbSelectConn; /** * 获取多个结果集，返回一个二维数组 * @param string $sql * @param array|null $bindParams * @return array * @throws \Exception */ public function getRows(string $sql, array $bindParams = null):array &#123; if (empty($sql)) &#123; throw new \Exception(&apos;sql异常:sql不能为空!&apos;); &#125; try &#123; $connect = $this-&gt;getReadConnection(); $query = $connect-&gt;query($sql, $bindParams); $query-&gt;setFetchMode(Db::FETCH_ASSOC); $data = $query-&gt;fetchAll(); &#125; catch (\Exception $e) &#123; $error = [&apos;code&apos; =&gt; $e-&gt;getCode(), &apos;msg&apos; =&gt; $e-&gt;getMessage(), &apos;file&apos; =&gt; $e-&gt;getFile(), &apos;line&apos; =&gt; $e-&gt;getLine()]; $error = json_encode($error); throw new \Exception(&quot;sql异常:&#123;$error&#125;&quot;); &#125; return $data; &#125; /** * 根据条件获取一条数据，返回一个一维数组 * @param string $sql * @param array $bindParams * @return array * @throws \Exception */ public function findOne(string $sql, array $bindParams = []):array &#123; if (empty($sql)) &#123; throw new \Exception(&apos;sql异常:sql不能为空!&apos;); &#125; try &#123; $connect = $this-&gt;getReadConnection(); $query = $connect-&gt;query($sql, $bindParams); $query-&gt;setFetchMode(Db::FETCH_ASSOC); $data = $query-&gt;fetchAll(); &#125; catch (\Exception $e) &#123; $error = [&apos;code&apos; =&gt; $e-&gt;getCode(), &apos;msg&apos; =&gt; $e-&gt;getMessage(), &apos;file&apos; =&gt; $e-&gt;getFile(), &apos;line&apos; =&gt; $e-&gt;getLine()]; $error = json_encode($error); throw new \Exception(&quot;sql异常:&#123;$error&#125;&quot;); &#125; return !empty($data) ? $data[0] : []; &#125; /** * 基础db操作方法 * 支持新增.修改 * * 新增 * $model 为 new Model() * 修改 * $model 为 Model::findFirst(&quot;id=$id&quot;); * * @param $model UserModel * @param $info * @return mixed */ public function baseSave($model, $info) &#123; try &#123; foreach ($info as $_key =&gt; $_v) &#123; $model-&gt;&#123;$_key&#125; = $_v; &#125; $rs = $model-&gt;save(); if ($rs === false) &#123; var_dump($model-&gt;getMessages()); exit; &#125; return $rs; &#125; catch (Exception $e) &#123; Logger::errors(&apos;base save sql : &apos; . $e-&gt;getMessage()); return false; &#125; &#125; public function conn(string $db) &#123; $this-&gt;dbSelectConn = $this-&gt;getDI()-&gt;getShared(&apos;config&apos;)-&gt;database-&gt;db_select_conn; /** @var object $conf */ $conf = $this-&gt;getDI()-&gt;getShared(&apos;config&apos;)-&gt;database-&gt;$db; /** @var array $confArr */ $confArr = $conf-&gt;toArray(); /** * 读写不分离的数据源配置 直接返回 数据 service */ if (isset($confArr[&apos;read&apos;]) == false &amp;&amp; isset($confArr[&apos;write&apos;]) == false) &#123; $dbServicePrefix = &apos;db_conn_&apos; . $db; if($this-&gt;getDI()-&gt;has($dbServicePrefix) == false)&#123; $this-&gt;getDI()-&gt;setShared($dbServicePrefix, function () use ($confArr) &#123; return new Mysql($confArr); &#125;); &#125; $this-&gt;setConnectionService($dbServicePrefix); return; &#125; /** * 读库数据源设置 * /** * db service 使用 key , 对应的是数据库配置的下标 * eg new_wd_read =&gt; 1 =&gt; rz vip_read_1 */ if (isset($confArr[&apos;read&apos;])) &#123; if (is_array(current($confArr[&apos;read&apos;])) == false) throw new \PDOException(&apos;请正确配置数据源&apos; . $db . &apos; 读库&apos;); $dbServicePrefix = $db.&quot;_read&quot;; $this-&gt;dbSelect($dbServicePrefix, $confArr[&apos;read&apos;], $dbServicePrefix,&apos;read&apos;); &#125; /** * 写库数据源设置 */ if (isset($confArr[&apos;write&apos;])) &#123; if (is_array(current($confArr[&apos;write&apos;])) == false) throw new \PDOException(&apos;请正确配置数据源&apos; . $db . &apos; 写库&apos;); $dbServicePrefix = $db.&quot;_write&quot;; $this-&gt;dbSelect($dbServicePrefix, $confArr[&apos;write&apos;], $dbServicePrefix,&apos;write&apos;); &#125; &#125; /** * 多库情况下 选择需要链接的数据库 * @param $db | 数据库 * @param $confArr * @param $dbServicePrefix * @param $type * @internal param $db_conn_count | 数据库连接配置数据 */ private function dbSelect($db, $confArr, $dbServicePrefix, $type) &#123; $dbConnCount = count($confArr); $serviceKey = md5($dbServicePrefix); if(isset($this-&gt;dbSelectConn[$db]) )&#123; $this-&gt;registerService($serviceKey,$type); return; &#125; try&#123; $rds = Redis::init()-&gt;incr(&apos;db_select_conn_&apos;.$db); $key = $rds % $dbConnCount; $this-&gt;dbSelectConn[$db] = $key; $this-&gt;setSharedService($confArr,$dbServicePrefix,$serviceKey); $this-&gt;registerService($serviceKey,$type); &#125;catch(Exception $e)&#123; Logger::errors( $e-&gt;getMessage() ); $this-&gt;dbSelectConn[$db] = mt_rand(0,$dbConnCount - 1); $this-&gt;setSharedService($confArr,$dbServicePrefix,$serviceKey); $this-&gt;registerService($serviceKey,$type); &#125; &#125; /** * 配置数据服务 * @param $confArr * @param $dbServicePrefix * @param $serviceKey */ private function setSharedService($confArr, $dbServicePrefix, $serviceKey) &#123; $confArrSet = array_values($confArr); $dbSelectConf = $confArrSet[$this-&gt;dbSelectConn[$dbServicePrefix]]; $this-&gt;getDI()-&gt;setShared($serviceKey, function () use ($dbSelectConf) &#123; return new Mysql($dbSelectConf); &#125;); &#125; /** * 设置数据连接 * @param $serviceKey * @param $type */ private function registerService($serviceKey, $type) &#123; if( $type == &apos;read&apos; )&#123; $this-&gt;setReadConnectionService($serviceKey); &#125;else&#123; $this-&gt;setWriteConnectionService($serviceKey); &#125; &#125;&#125; 配置config1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;?phpreturn [ &apos;db_select_conn&apos; =&gt; [], &apos;w_database&apos; =&gt; [ &apos;adapter&apos; =&gt; &apos;Mysql&apos;, &apos;host&apos; =&gt; &apos;127.0.0.1&apos;, &apos;username&apos; =&gt; &apos;root&apos;, &apos;password&apos; =&gt; &apos;&apos;, &apos;dbname&apos; =&gt; &apos;rz_vip&apos;, &apos;port&apos; =&gt; &apos;4011&apos; ], &apos;rz_vip&apos; =&gt; [ &apos;read&apos;=&gt;[ &apos;vip_read_1&apos; =&gt; [ &apos;adapter&apos; =&gt; &apos;Mysql&apos;, &apos;host&apos; =&gt; &apos;127.0.0.1&apos;, &apos;username&apos; =&gt; &apos;root&apos;, &apos;password&apos; =&gt; &apos;&apos;, &apos;dbname&apos; =&gt; &apos;rz_vip&apos;, &apos;port&apos; =&gt; &apos;4011&apos; ], &apos;vip_read_2&apos; =&gt; [ &apos;adapter&apos; =&gt; &apos;Mysql&apos;, &apos;host&apos; =&gt; &apos;127.0.0.1&apos;, &apos;username&apos; =&gt; &apos;root&apos;, &apos;password&apos; =&gt; &apos;&apos;, &apos;dbname&apos; =&gt; &apos;rz_vip&apos;, &apos;port&apos; =&gt; &apos;4011&apos; ], ], &apos;write&apos;=&gt;[ [ &apos;adapter&apos; =&gt; &apos;Mysql&apos;, &apos;host&apos; =&gt; &apos;127.0.0.1&apos;, &apos;username&apos; =&gt; &apos;root&apos;, &apos;password&apos; =&gt; &apos;&apos;, &apos;dbname&apos; =&gt; &apos;rz_vip&apos;, &apos;port&apos; =&gt; &apos;4011&apos; ] ] ], &apos;new_wd&apos; =&gt; [ &apos;read&apos;=&gt;[ &apos;vip_read_1&apos; =&gt; [ &apos;adapter&apos; =&gt; &apos;Mysql&apos;, &apos;host&apos; =&gt; &apos;127.0.0.1&apos;, &apos;username&apos; =&gt; &apos;root&apos;, &apos;password&apos; =&gt; &apos;&apos;, &apos;dbname&apos; =&gt; &apos;new_wd&apos;, &apos;port&apos; =&gt; &apos;4011&apos; ], &apos;vip_read_2&apos; =&gt; [ &apos;adapter&apos; =&gt; &apos;Mysql&apos;, &apos;host&apos; =&gt; &apos;127.0.0.1&apos;, &apos;username&apos; =&gt; &apos;root&apos;, &apos;password&apos; =&gt; &apos;&apos;, &apos;dbname&apos; =&gt; &apos;new_wd&apos;, &apos;port&apos; =&gt; &apos;4011&apos; ], ], &apos;write&apos;=&gt;[ [ &apos;adapter&apos; =&gt; &apos;Mysql&apos;, &apos;host&apos; =&gt; &apos;127.0.0.1&apos;, &apos;username&apos; =&gt; &apos;root&apos;, &apos;password&apos; =&gt; &apos;&apos;, &apos;dbname&apos; =&gt; &apos;new_wd&apos;, &apos;port&apos; =&gt; &apos;4011&apos; ], [ &apos;adapter&apos; =&gt; &apos;Mysql&apos;, &apos;host&apos; =&gt; &apos;127.0.0.1&apos;, &apos;username&apos; =&gt; &apos;root&apos;, &apos;password&apos; =&gt; &apos;&apos;, &apos;dbname&apos; =&gt; &apos;new_wd&apos;, &apos;port&apos; =&gt; &apos;4011&apos; ] ] ],];]]></content>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phalcon log date format]]></title>
    <url>%2F2018%2F01%2F01%2Fphp%2Fphalcon-log-date-format%2F</url>
    <content type="text"><![CDATA[#phalcon log date format ##phalcon 日志日期格式重写 12345$loger = new File(&apos;/data/logs/test.log&apos;);$logDateFormat = new Line();$logDateFormat-&gt;setDateFormat(&apos;Y-m-d H:i:s&apos;);$loger-&gt;setFormatter($logDateFormat);$loger-&gt;alert(&apos;test &apos;.time()); ##日志注册到服务中 ###设置 log handle 12345678910111213141516171819202122232425use Phalcon\Di;$di = new \Phalcon\DI\FactoryDefault();$di-&gt;setShared(&apos;loger&apos;, function () &#123; $logRootPath = Di::getDefault()-&gt;get(&apos;config&apos;)-&gt;logger-&gt;log_root_path; $date = \Common\Functions::date(1); $filePath = $logRootPath . $date . DIRECTORY_SEPARATOR; if (is_dir($filePath) === false) &#123; mkdir($filePath, 0777, true); &#125; $loger = new File($filePath . &apos;/hello.log&apos;); $logDateFormat = new Line(); $logDateFormat-&gt;setDateFormat(&apos;Y-m-d H:i:s&apos;); $loger-&gt;setFormatter($logDateFormat); Di::getDefault()-&gt;set(&apos;logerInstance&apos;, $loger); return $loger; &#125;); /** * 用户级别日志捕获 */ set_error_handler(function ($errno, $errInfo, $errFile, $errLine) &#123; Di::getDefault()-&gt;get(&apos;loger&apos;)-&gt;info(sprintf(&apos;%s %s %s&apos;, $errFile, $errLine, $errInfo)); &#125;); 调用1triggen_error( &apos;test&apos;.time() );]]></content>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK6.0 KAFKA 搭建]]></title>
    <url>%2F2017%2F12%2F10%2Felk%2FELK6.0%20KAFKA%20%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[ELK6.0 KAFKA 搭建 [toc] ELK下载(版本统一6.0.0):filebeatelasticsearchlogstashx-pack 配置文件因为公司信息安全原因 暂不提供 有需要的可以私聊我elasticsearch.ymlkafka_test.confkibana.confkibana.ymlkibana-nginx.conf 插件下载logstash-input-kafka 依赖关系:ruby(&gt;2.0.0)123456789101112131415sudo yum remove ruby ruby-develsudo yum groupinstall &quot;Development Tools&quot;sudo yum install openssl-develwget http://cache.ruby-lang.org/pub/ruby/2.1/ruby-2.1.2.tar.gztar xvfvz ruby-2.1.2.tar.gz &amp;&amp; cd ruby-2.1.2./configuremake &amp;&amp; make install-- upgrangesudo gem update --systemsudo gem install bundler--checkruby --versionrubygems --version rvm12345\curl -sSL https://get.rvm.io | bashrvm install jruby-1.7.19rvm use jruby-1.7.19gem install bundlerbundle install 插件本地安装命令12vim /root/logstash-6.0.0/Gemfilegem &quot;logstash-input-kafka&quot;, :path =&gt; &quot;/root/logstash-input-kafka-master&quot; 1/root/logstash-6.0.0/bin/logstash-plugin update 命令配置1234567891011121314 vim ~/.bash_cmd alias filebeat=&quot;nohup /root/filebeat-6.0.0-linux-x86_64/filebeat -e -c /root/filebeat-6.0.0-linux-x86_64/filebeat.yml -d &quot;publish&quot; &amp;&gt;&gt;/data/log/filebeat.log &amp;&quot;alias filebeat_kill=&quot;ps -ef | grep filebeat | grep -v grep | awk &apos;&#123;print $2&#125;&apos; | xargs kill&quot;alias kibana=&quot;nohup /root/kibana-6.0.0-linux-x86_64/bin/kibana &amp;&gt;&gt;/data/log/kibana.log &amp;&quot;alias kibana_kill=&quot;ps -ef | grep kibana | grep -v grep | awk &apos;&#123;print $2&#125;&apos; | xargs kill&quot;alias logstash_kafka=&quot;nohup /root/logstash-6.0.0/bin/logstash -f /root/logstash-6.0.0/config/kafka_test.conf &amp;&gt;&gt;/data/log/logstash-kafka.log &amp;&quot;alias logstash_kafka_kill=&quot;ps -ef | grep kafka_test.conf | grep -v grep | awk &apos;&#123;print $2&#125;&apos; | xargs kill&quot;alias elastic=&quot;nohup /home/es/elasticsearch-6.0.0/bin/elasticsearch &amp;&gt;&gt;/data/log/elastic.log &amp;&quot;alias elastic_kill=&quot;ps -ef | grep elasticsearch | grep -v grep | awk &apos;&#123;print $2&#125;&apos; | xargs kill&quot; kafka下载(1.0.0)kafka1231.java.net.UnknownHostException: test-185: test-185: 未知的名称或服务vim /etc/hosts 添加: `10.1.1.1 test-185` php 扩展下载rdkafka 3.0.5zlib 1.2.7zookeeper 0.4.0 (libzookeeper version 3.4.10) composer 库依赖下载1234567891011&#123; &quot;require-dev&quot;: &#123; &quot;eaglewu/swoole-ide-helper&quot;: &quot;dev-master&quot;, &quot;sneakybobito/phalcon-stubs&quot;: &quot;3.0.1&quot;, &#125;, &quot;require&quot;: &#123; &quot;monolog/monolog&quot;: &quot;^1.23&quot;, &quot;propel/propel&quot;: &quot;~2.0@dev&quot;, &quot;kwn/php-rdkafka-stubs&quot;: &quot;^1.0&quot; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[php-redis-cli]]></title>
    <url>%2F2017%2F05%2F16%2Fredis%2Fphp-redis-cli%2F</url>
    <content type="text"><![CDATA[#php redis cli ##建立redis集群最近公司的redis主从系统崩溃了，然后开始研究下redis的集群redis的集群比较简单，或者说是redis官方目前提供的集群方案应非常成熟了： 建立集群：12345cd redis-3.2.8/utils/create-cluster/./create-cluster start./create-cluster create 完成。详细的redis集群搭建可以参考我的另一篇博客。 ##phprediscli 选择合适的redis cli 以predis为例 从githup clone项目 通过composer解决项目加载 依赖 的问题 composer install -vvv 代码实现 12345678910111213141516171819202122include &apos;autoload.php&apos;;$parameters = [&apos;tcp://127.0.0.1:3000&apos;];$options = [&apos;cluster&apos; =&gt; &apos;redis&apos;];$client = new Predis\Client($parameters, $options);$time = explode(&apos; &apos;,microtime());$start = $time[1].substr($time[0],1);echo $start . PHP_EOL;for ($i = 0; $i &lt; 100000; $i++) &#123; $key = mt_rand(1000000, 2000000); $client-&gt;set(&apos;KEY&apos; . $key, &apos;VALUE:&apos; . $key);&#125;$client-&gt;quit();$time = explode(&apos; &apos;,microtime());$end = $time[1].substr($time[0],1);echo $end . PHP_EOL;echo &apos;一共耗时：&apos; .bcsub($end,$start,3); 在写入大概1000万数据后，开始出现的错误，因为我配置开aof。。19276:M 16 May 15:36:28.060 * Asynchronous AOF fsync is taking too long (disk is busy?). Writing the AOF buffer without waiting for fsync to complete, this may slow down Redis.]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql优化]]></title>
    <url>%2F2017%2F05%2F12%2Fmysql%2Fsql%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[#mysql 优化策略 目前看到喜欢的mysql优化策略来自美团点评技术团队 防身的mysql优化策略： 索引 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录 索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’); 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可 更省力的索引检查还是美团点评技术团队的case： 用例如下，适合对mysql优化无力深入的。。。。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667./sqladvisor -h dev-be-mysql.co904fphklgb.ap-southeast-1.rds.amazonaws.com -P 3306 -u langapi -p &apos;!ang!iv*b*&apos; -d db_billing -q &quot;SELECT ca.tid, ca.pfid, ca.cash, ca.gold, ca.order_time, ca.status, ca.op_id, ca.op_time, ca.to_account, ch.name, ch.phone, ch.birthday, ch.address, ch.id_card_address, ch.id_card_img_0, ch.id_card_img_1, ch.bank_card_img, ch.bank_code, ch.bank_name, ch.email, ch.bank_branch_name, cr.company_id, cp.company AS company, ch.cashier_type, ch.op_id AS cashier_op_id FROM tb_cashier AS ca LEFT JOIN tb_cashier_hot AS ch ON ca.tid = ch.tid LEFT JOIN tb_company_relationship AS cr ON cr.pfid = ca.pfid LEFT JOIN tb_company AS cp ON cp.id = cr.company_id LIMIT 100&quot; -v 12017-05-12 17:48:12 31770 [Note] 第1步: 对SQL解析优化之后得到的SQL:select `ca`.`tid` AS `tid`,`ca`.`pfid` AS `pfid`,`ca`.`cash` AS `cash`,`ca`.`gold` AS `gold`,`ca`.`order_time` AS `order_time`,`ca`.`status` AS `status`,`ca`.`op_id` AS `op_id`,`ca`.`op_time` AS `op_time`,`ca`.`to_account` AS `to_account`,`ch`.`name` AS `name`,`ch`.`phone` AS `phone`,`ch`.`birthday` AS `birthday`,`ch`.`address` AS `address`,`ch`.`id_card_address` AS `id_card_address`,`ch`.`id_card_img_0` AS `id_card_img_0`,`ch`.`id_card_img_1` AS `id_card_img_1`,`ch`.`bank_card_img` AS `bank_card_img`,`ch`.`bank_code` AS `bank_code`,`ch`.`bank_name` AS `bank_name`,`ch`.`email` AS `email`,`ch`.`bank_branch_name` AS `bank_branch_name`,`cr`.`company_id` AS `company_id`,`cp`.`company` AS `company`,`ch`.`cashier_type` AS `cashier_type`,`ch`.`op_id` AS `cashier_op_id` from (((`db_billing`.`tb_cashier` `ca` left join `db_billing`.`tb_cashier_hot` `ch` on((`ca`.`tid` = `ch`.`tid`))) left join `db_billing`.`tb_company_relationship` `cr` on((`cr`.`pfid` = `ca`.`pfid`))) le2017-05-12 17:48:12 31770 [Note] 第2步：开始解析join on条件:ca.tid=ch.tid 2017-05-12 17:48:12 31770 [Note] 第3步：开始解析join on条件:cr.pfid=ca.pfid 2017-05-12 17:48:12 31770 [Note] 第4步：开始解析join on条件:cp.id=cr.company_id 2017-05-12 17:48:12 31770 [Note] 第5步：开始选择驱动表,一共有1个候选驱动表 2017-05-12 17:48:12 31770 [Note] explain select * from tb_cashier 2017-05-12 17:48:12 31770 [Note] 第6步：候选驱动表tb_cashier的结果集行数为:182 2017-05-12 17:48:12 31770 [Note] 第7步：选择表tb_cashier为驱动表 2017-05-12 17:48:12 31770 [Note] 第8步：表tb_cashier 的SQL太逆天,没有优化建议 2017-05-12 17:48:12 31770 [Note] 第9步：开始验证 字段tid是不是主键。表名:tb_cashier_hot 2017-05-12 17:48:12 31770 [Note] show index from tb_cashier_hot where Key_name = &apos;PRIMARY&apos; and Column_name =&apos;tid&apos; and Seq_in_index = 1 2017-05-12 17:48:12 31770 [Note] 第10步：字段tid不是主键。表名:tb_cashier_hot 2017-05-12 17:48:12 31770 [Note] 第11步：开始验证 字段tid是不是主键。表名:tb_cashier_hot 2017-05-12 17:48:12 31770 [Note] show index from tb_cashier_hot where Key_name = &apos;PRIMARY&apos; and Column_name =&apos;tid&apos; and Seq_in_index = 1 2017-05-12 17:48:12 31770 [Note] 第12步：字段tid不是主键。表名:tb_cashier_hot 2017-05-12 17:48:12 31770 [Note] 第13步：开始验证表中是否已存在相关索引。表名:tb_cashier_hot, 字段名:tid, 在索引中的位置:1 2017-05-12 17:48:12 31770 [Note] show index from tb_cashier_hot where Column_name =&apos;tid&apos; and Seq_in_index =1 2017-05-12 17:48:12 31770 [Note] 第14步：开始输出表tb_cashier_hot索引优化建议: 2017-05-12 17:48:12 31770 [Note] Create_Index_SQL：alter table tb_cashier_hot add index idx_tid(tid) 2017-05-12 17:48:12 31770 [Note] 第15步：开始验证 字段pfid是不是主键。表名:tb_company_relationship 2017-05-12 17:48:12 31770 [Note] show index from tb_company_relationship where Key_name = &apos;PRIMARY&apos; and Column_name =&apos;pfid&apos; and Seq_in_index = 1 2017-05-12 17:48:12 31770 [Note] 第16步：字段pfid不是主键。表名:tb_company_relationship 2017-05-12 17:48:12 31770 [Note] 第17步：开始验证 字段pfid是不是主键。表名:tb_company_relationship 2017-05-12 17:48:12 31770 [Note] show index from tb_company_relationship where Key_name = &apos;PRIMARY&apos; and Column_name =&apos;pfid&apos; and Seq_in_index = 1 2017-05-12 17:48:12 31770 [Note] 第18步：字段pfid不是主键。表名:tb_company_relationship 2017-05-12 17:48:12 31770 [Note] 第19步：开始验证表中是否已存在相关索引。表名:tb_company_relationship, 字段名:pfid, 在索引中的位置:1 2017-05-12 17:48:12 31770 [Note] show index from tb_company_relationship where Column_name =&apos;pfid&apos; and Seq_in_index =1 2017-05-12 17:48:12 31770 [Note] 第20步：索引(pfid)已存在 2017-05-12 17:48:12 31770 [Note] 第21步：开始验证 字段id是不是主键。表名:tb_company 2017-05-12 17:48:12 31770 [Note] show index from tb_company where Key_name = &apos;PRIMARY&apos; and Column_name =&apos;id&apos; and Seq_in_index = 1 2017-05-12 17:48:12 31770 [Note] 第22步：字段id是主键。表名:tb_company 2017-05-12 17:48:12 31770 [Note] 第23步：表tb_company 经过运算得到的索引列首列是主键,直接放弃,没有优化建议 2017-05-12 17:48:12 31770 [Note] 第24步: SQLAdvisor结束!]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[session解析]]></title>
    <url>%2F2017%2F05%2F12%2Fsession%2Fcookie%E7%A6%81%E7%94%A8%E6%8E%89%E4%B9%8B%E5%90%8E%2F</url>
    <content type="text"><![CDATA[#session解析 ##httpHttp是一种无状态性的协议。这是因为此种协议不要求浏览器在每次请求中标明它自己的身份，并且浏览器以及服务器之间并没有保持一个持久性的连接用于多个页面之间的访问。当一个用户访问一个站点的时候，用户的浏览器发送一个http请求到服务器，服务器返回给浏览器一个http响应。其实很简单的一个概念，客户端一个请求，服务器端一个回复，这就是整个基于http协议的通讯过程。 ##cookieCookie（复数形态Cookies），中文名称为“小型文本文件”或“小甜饼”[1]，指某些网站为了辨别用户身份而储存在用户本地终端（Client Side）上的数据（通常经过加密）。定义于RFC2109。是网景公司的前雇员卢·蒙特利在1993年3月的发明[2]。 浏览器中cookie ##禁用cookie之后 当禁用cookie之后，浏览器进行网页的访问，cookie就不存在了，自然不能将session_id传入到服务端，服务端也就无法识别用户的身份。 这里面的核心问题其实跟cookie没关系，仅仅是session_id（用户身份标识）无法传输到服务端了。 那我们把session_id作为一个参数来看待，解决的问题的方案就出来了，手动的传输我们session_id到服务端就好了。 这就是url重写。不管是get 还是post 请求，根据业务相应的传输方案就好。 当前端里面的session_id，通过请求发送到服务器之后，php服务端处理方案是： 获取客户端传入的session_id, 设置session_id 即可正常的读取session内容 12345678910111213141516171819&lt;?phpsession_start();session_id('020e163uku6ptt7cm6eoj50992');var_dump($_SESSION);if (empty($_SESSION['count'])) &#123; $_SESSION['count'] = 1;&#125; else &#123; $_SESSION['count']++;&#125;?&gt;&lt;p&gt; Hello visitor, you have seen this page &lt;?php echo $_SESSION['count'].htmlspecialchars(SID); ?&gt; times.&lt;/p&gt;&lt;p&gt; To continue, &lt;a href="&lt;?php echo "nextpage.php?". htmlspecialchars(SID); ?&gt;"&gt;click here&lt;/a&gt;.&lt;/p&gt; 上述代码 SID的session_id，在cookie被禁用的模式下，会自动被赋值session_id($sid)，通过手动设置session_id 既可以实现session的状态 ##php 中的 session php中session的位置可以进行配置：session.save_path = “/tmp/session” 我使用的服务器，默认在/tmp里面session在服务器上的实际存在情况|字段|值||-:-||path |/tmp/sess_ucqqv1ej1lulttmste47hpuj56||info|$key\|type:len:&quot;$value&quot;\ 参考文档： SESSION 原理]]></content>
      <tags>
        <tag>session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis集群]]></title>
    <url>%2F2017%2F05%2F05%2Fredis%2Fredis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[#redis[toc] ##1. 基础配置参数 redis基础配置 字段 配置 eg prot 端口 7379 daemonize yes 后台执行 cluster-enabled yes 开启集群 cluster-config-file nodes.conf 集群配置文件，自动生成 cluster-node-timeout 5000 集群中节点状态检查超时时间，超过界限会被标记为失败，并通过选举算法，关闭节点 cluster-slave-validity-factor 尝试主从切换的时间 为0时始终常识进行主从切换 ；请注意，任何不同于零的值都可能导致如果没有从站能够故障转移，则主站故障后Redis Cluster将不可用 logfile “redis.log” log文件配置 cluster-migration-barrier ? cluster-require-full-coverage 集群空间全覆盖检查? 如果这设置为是，默认情况下，如果某个百分比的密钥空间未被任何节点覆盖，则群集将停止接受写入，如果选项设置为否，则集群仍将提供查询，即使仅可以处理关于键子集的请求。 dbfilename save db on disk dbfilename 7379.rdb 最快1分钟备份一次 默认的备份方式 appendonly yes aof持久化开启，有较大的io压力，谨慎开启 appendfilename save cmd 7379.appendonly.aof 每条命令执行进行备份；io压力 appendfsync always everysec no 递减 ##2. 高可用 ###架构目标 Cluster + (master &amp; slaves)可选方案 redis集群 主从 sentinel 2 ###集群数据一致性保障 异步操作，不能保证强一致性。 节点超时 ##3. 集群创建的流程 请以官方文档为主：https://redis.io/topics/cluster-tutorial 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162** redis.conf文件port 7000daemonize yescluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000appendonly yeslogfile "redis.log"** 启动集群 输出vagrant@vagrant-ubuntu-trusty-64:~$ /data/local/redis/redis-3.2.8/src/redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:127.0.0.1:7000127.0.0.1:7001127.0.0.1:7002Adding replica 127.0.0.1:7003 to 127.0.0.1:7000Adding replica 127.0.0.1:7004 to 127.0.0.1:7001Adding replica 127.0.0.1:7005 to 127.0.0.1:7002M: fb165309530288508d643b45381c114ac23246e7 127.0.0.1:7000 slots:0-5460 (5461 slots) masterM: 849c0c5c3401cd429cfc12943773c8c5c3878b63 127.0.0.1:7001 slots:5461-10922 (5462 slots) masterM: a9be2c77dd3c67728315eba597dd13ce5ecd7523 127.0.0.1:7002 slots:10923-16383 (5461 slots) masterS: e8addb041027ab38ef58a1e2a5cfa920ff188fca 127.0.0.1:7003 replicates fb165309530288508d643b45381c114ac23246e7S: b5d559e19d2a539b16d5b0cb05743a24865854cf 127.0.0.1:7004 replicates 849c0c5c3401cd429cfc12943773c8c5c3878b63S: 97364828304bcff993cddd696c631f79eeaad7f1 127.0.0.1:7005 replicates a9be2c77dd3c67728315eba597dd13ce5ecd7523Can I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: fb165309530288508d643b45381c114ac23246e7 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: b5d559e19d2a539b16d5b0cb05743a24865854cf 127.0.0.1:7004 slots: (0 slots) slave replicates 849c0c5c3401cd429cfc12943773c8c5c3878b63S: 97364828304bcff993cddd696c631f79eeaad7f1 127.0.0.1:7005 slots: (0 slots) slave replicates a9be2c77dd3c67728315eba597dd13ce5ecd7523M: 849c0c5c3401cd429cfc12943773c8c5c3878b63 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: e8addb041027ab38ef58a1e2a5cfa920ff188fca 127.0.0.1:7003 slots: (0 slots) slave replicates fb165309530288508d643b45381c114ac23246e7M: a9be2c77dd3c67728315eba597dd13ce5ecd7523 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 节点id创建的时候确定。不随着ip port 改变； ###集群中客户端 输出123456789101112131414079:M 04 May 07:44:31.314 # Server started, Redis version 3.2.814079:M 04 May 07:44:31.315 * The server is now ready to accept connections on port 700014079:M 04 May 07:52:02.099 # configEpoch set to 1 via CLUSTER SET-CONFIG-EPOCH14079:M 04 May 07:52:02.119 # IP address for this node updated to 127.0.0.114079:M 04 May 07:52:06.529 * Slave 127.0.0.1:7003 asks for synchronization14079:M 04 May 07:52:06.529 * Full resync requested by slave 127.0.0.1:700314079:M 04 May 07:52:06.529 * Starting BGSAVE for SYNC with target: disk14079:M 04 May 07:52:06.529 * Background saving started by pid 1467714677:C 04 May 07:52:06.531 * DB saved on disk14677:C 04 May 07:52:06.532 * RDB: 0 MB of memory used by copy-on-write14079:M 04 May 07:52:06.626 * Background saving terminated with success14079:M 04 May 07:52:06.628 * Synchronization with slave 127.0.0.1:7003 succeeded14079:M 04 May 07:52:07.032 # Cluster state changed: ok ###数据重塑-分配插槽 Resharding the cluster 所有的重塑抽取的都是从节点的前面slot开始抽取。 123456789开始重新塑造/data/local/redis/redis-3.2.8/src/redis-trib.rb reshard 127.0.0.1:7000 检查塑造结果/data/local/redis/redis-3.2.8/src/redis-trib.rb check 127.0.0.1:7000 查看自己的节点redis-cli -p 7000 cluster nodes | grep myself ###数据重塑-分配插槽 Scripting a resharding operation1234./redis-trib.rb reshard --from &lt;node-id&gt; --to &lt;node-id&gt; --slots &lt;number of slots&gt; --yes &lt;host&gt;:&lt;port&gt;eg./redis-trib.rb reshard --from fb165309530288508d643b45381c114ac23246e7 --to 849c0c5c3401cd429cfc12943773c8c5c3878b63 --slots 500 --yes 127.0.0.1:7000 ####集群重塑源文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849vagrant@vagrant-ubuntu-trusty-64:~$ /data/local/redis/redis-3.2.8/src/redis-trib.rb reshard 127.0.0.1:7000&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: fb165309530288508d643b45381c114ac23246e7 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: b5d559e19d2a539b16d5b0cb05743a24865854cf 127.0.0.1:7004 slots: (0 slots) slave replicates 849c0c5c3401cd429cfc12943773c8c5c3878b63S: 97364828304bcff993cddd696c631f79eeaad7f1 127.0.0.1:7005 slots: (0 slots) slave replicates a9be2c77dd3c67728315eba597dd13ce5ecd7523M: 849c0c5c3401cd429cfc12943773c8c5c3878b63 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: e8addb041027ab38ef58a1e2a5cfa920ff188fca 127.0.0.1:7003 slots: (0 slots) slave replicates fb165309530288508d643b45381c114ac23246e7M: a9be2c77dd3c67728315eba597dd13ce5ecd7523 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 1000What is the receiving node ID? fb165309530288508d643b45381c114ac23246e7Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs.Source node #1:all分别从其它节点中取得所需的slotsReady to move 1000 slots. Source nodes: M: 849c0c5c3401cd429cfc12943773c8c5c3878b63 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s) M: a9be2c77dd3c67728315eba597dd13ce5ecd7523 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s) Destination node: M: fb165309530288508d643b45381c114ac23246e7 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s) Resharding plan: Moving slot 5461 from 849c0c5c3401cd429cfc12943773c8c5c3878b63 Moving slot 5462 from 849c0c5c3401cd429cfc12943773c8c5c3878b63 ###故障转移 123456789101112131415161718192021查看所有的主节点 redis-cli -p 7000 cluster nodes | grep master主动出发销毁 redis-cli -p 7002 debug segfault查看销毁后的节点情况redis-cli -p 7000 cluster nodes集群节点信息The output of the CLUSTER NODES command may look intimidating, but it is actually pretty simple, and is composed of the following tokens:Node IDip:portflags: master, slave, myself, fail, ...if it is a slave, the Node ID of the masterTime of the last pending PING still waiting for a reply.Time of the last PONG received.Configuration epoch for this node (see the Cluster specification). 节点的配置时期？Status of the link to this node. 指向此节点的连接状态Slots served... 插槽分配 ###故障转移（Manual failover） Manual failovers are supported by Redis Cluster using the CLUSTER FAILOVER command, that must be executed in one of the slaves of the master you want to failover.手动故障转移再升级系统时非常有效。必须在预进行故障转移的从库中进行。 ####log文件123456# Manual failover user request accepted.# Received replication offset for paused master manual failover: 347540# All master replication stream processed, manual failover can start.# Start of election delayed for 0 milliseconds (rank #0, offset 347540).# Starting a failover election for epoch 7545.# Failover election won: I'm the new master. ###添加新的节点Adding a new node1234567随机添加（添加7006端口的机器，到7000集群中，默认添加的会是主库）/data/local/redis/redis-3.2.8/src/redis-trib.rb add-node 127.0.0.1:7006 127.0.0.1:7000指定添加/data/local/redis/redis-3.2.8/src/redis-trib.rb add-node --slave --master-id 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7006 127.0.0.1:7000 ####源文件123456789101112131415161718192021222324vagrant@vagrant-ubuntu-trusty-64:~$ /data/local/redis/redis-3.2.8/src/redis-trib.rb add-node 127.0.0.1:7006 127.0.0.1:7000&gt;&gt;&gt; Adding node 127.0.0.1:7006 to cluster 127.0.0.1:7000&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: fb165309530288508d643b45381c114ac23246e7 127.0.0.1:7000 slots:500-5961,10923-11421 (5961 slots) master 1 additional replica(s)S: b5d559e19d2a539b16d5b0cb05743a24865854cf 127.0.0.1:7004 slots: (0 slots) slave replicates 849c0c5c3401cd429cfc12943773c8c5c3878b63M: 97364828304bcff993cddd696c631f79eeaad7f1 127.0.0.1:7005 slots:11422-16383 (4962 slots) master 0 additional replica(s)M: 849c0c5c3401cd429cfc12943773c8c5c3878b63 127.0.0.1:7001 slots:0-499,5962-10922 (5461 slots) master 1 additional replica(s)S: e8addb041027ab38ef58a1e2a5cfa920ff188fca 127.0.0.1:7003 slots: (0 slots) slave replicates fb165309530288508d643b45381c114ac23246e7[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 127.0.0.1:7006 to make it join the cluster.[OK] New node added correctly. ####手动添加的状态123vagrant@vagrant-ubuntu-trusty-64:~$ redis-cli -c -p 7006127.0.0.1:7006&gt; cluster nodesb322c8631fe98c26e7e26b5cd8f8d1e3b32da16b 127.0.0.1:7006 myself,master - 0 0 0 connected ###为集群自动添加从库 Adding a new node as a replicacluster replicate 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e ###删除节点 12redis-trib del-node 127.0.0.1:7000 b322c8631fe98c26e7e26b5cd8f8d1e3b32da16b 如果需要删除的节点重新作为从库加入到集群中，需要删除node rm -rf nodes.conf删除主节点时，需要讲主节点的数据清空：参考手动故障转移 ###副本迁移 Replicas migration12CLUSTER REPLICATE &lt;master-node-id&gt; 死掉的主库，在被重新拉起之后，自动顶替它的主库的从库 ##redis配置文件 完整参考12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option "include" won't be rewritten by command "CONFIG REWRITE"# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you'd better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## NETWORK ###################################### By default, if no "bind" configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the "bind" configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 lookback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 127.0.0.1# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# "bind" directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the "bind" directive.protected-mode yes# Accept connections on the specified port, default is 7379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 7379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# GENERAL ###################################### By default Redis does not run as a daemon. Use 'yes' if you need it.# Note that Redis will write a pid file in /usr/local/var/run/redis.pid when daemonized.daemonize yes# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal "process is ready."# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to "/usr/local/var/run/redis.pid".## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_7379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile "/data/log/redis.7379.log"# To enable logging to the system logger, just set 'syslog-enabled' to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and 'databases'-1databases 16################################ SNAPSHOTTING ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all "save" lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save ""save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that's set to 'yes' as it's almost always a win.# If you want to save some CPU in the saving child set it to 'no' but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename 7379.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the 'dbfilename' configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir /usr/local/var/db/redis/################################# REPLICATION ################################## Master-Slave replication. Use slaveof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of slaves.# 2) Redis slaves are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition slaves automatically try to reconnect to masters# and resynchronize with them.## slaveof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the "requirepass" configuration# directive below) it is possible to tell the slave to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the slave request.## masterauth &lt;master-password&gt;# When a slave loses its connection with the master, or when the replication# is still in progress, the slave can act in two different ways:## 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if slave-serve-stale-data is set to 'no' the slave will reply with# an error "SYNC with master in progress" to all the kind of commands# but to INFO and SLAVEOF.#slave-serve-stale-data yes# You can configure a slave instance to accept writes or not. Writing against# a slave instance may be useful to store some ephemeral data (because data# written on a slave will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default slaves are read-only.## Note: read only slaves are not designed to be exposed to untrusted clients# on the internet. It's just a protection layer against misuse of the instance.# Still a read only slave exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only slaves using 'rename-command' to shadow all the# administrative / dangerous commands.slave-read-only yes# Replication SYNC strategy: disk or socket.## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## New slaves and reconnecting slaves that are not able to continue the replication# process just receiving differences, need to do what is called a "full# synchronization". An RDB file is transmitted from the master to the slaves.# The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the slaves incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to slave sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more slaves# can be queued and served with the RDB file as soon as the current child producing# the RDB file finishes its work. With diskless replication instead once# the transfer starts, new slaves arriving will be queued and a new transfer# will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple slaves# will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the slaves.## This is important since once the transfer starts, it is not possible to serve# new slaves arriving, that will be queued for the next RDB transfer, so the server# waits a delay in order to let more slaves arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# Slaves send PINGs to server in a predefined interval. It's possible to change# this interval with the repl_ping_slave_period option. The default value is 10# seconds.## repl-ping-slave-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of slave.# 2) Master timeout from the point of view of slaves (data, pings).# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-slave-period otherwise a timeout will be detected# every time there is low traffic between the master and the slave.## repl-timeout 60# Disable TCP_NODELAY on the slave socket after SYNC?## If you select "yes" Redis will use a smaller number of TCP packets and# less bandwidth to send data to slaves. But this can add a delay for# the data to appear on the slave side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select "no" the delay for data to appear on the slave side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and slaves are many hops away, turning this to "yes" may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# slave data when slaves are disconnected for some time, so that when a slave# wants to reconnect again, often a full resync is not needed, but a partial# resync is enough, just passing the portion of data the slave missed while# disconnected.## The bigger the replication backlog, the longer the time the slave can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a slave connected.## repl-backlog-size 1mb# After a master has no longer connected slaves for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last slave disconnected, for# the backlog buffer to be freed.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The slave priority is an integer number published by Redis in the INFO output.# It is used by Redis Sentinel in order to select a slave to promote into a# master if the master is no longer working correctly.## A slave with a low priority number is considered better for promotion, so# for instance if there are three slaves with priority 10, 100, 25 Sentinel will# pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the slave as not able to perform the# role of master, so a slave with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.slave-priority 100# It is possible for a master to stop accepting writes if there are less than# N slaves connected, having a lag less or equal than M seconds.## The N slaves need to be in "online" state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the slave, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough slaves# are available, to the specified number of seconds.## For example to require at least 3 slaves with a lag &lt;= 10 seconds use:## min-slaves-to-write 3# min-slaves-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-slaves-to-write is set to 0 (feature disabled) and# min-slaves-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# slaves in different ways. For example the "INFO replication" section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover slave instances.# Another place where this info is available is in the output of the# "ROLE" command of a masteer.## The listed IP and address normally reported by a slave is obtained# in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the slave to connect with the master.## Port: The port is communicated by the slave during the replication# handshake, and is normally the port that the slave is using to# list for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the slave may be actually reachable via different IP and port# pairs. The following two options can be used by a slave in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## slave-announce-ip 5.5.5.5# slave-announce-port 1234################################## SECURITY #################################### Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other# commands. This might be useful in environments in which you do not trust# others with access to the host running redis-server.## This should stay commented out for backward compatibility and because most# people do not need auth (e.g. they run their own servers).## Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## requirepass foobared# Command renaming.## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG ""## Please note that changing the name of commands that are logged into the# AOF file or transmitted to slaves may cause problems.################################### LIMITS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error 'max number of clients reached'.## maxclients 10000# Don't use more memory than the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can't remove keys according to the policy, or if the policy is# set to 'noeviction', Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU cache, or to set# a hard memory limit for an instance (using the 'noeviction' policy).## WARNING: If you have slaves attached to an instance with maxmemory on,# the size of the output buffers needed to feed the slaves are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of slaves is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have slaves attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for slave# output buffers (but this is not needed if the policy is 'noeviction').## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select among five behaviors:## volatile-lru -&gt; remove the key with an expire set using an LRU algorithm# allkeys-lru -&gt; remove any key according to the LRU algorithm# volatile-random -&gt; remove a random key with an expire set# allkeys-random -&gt; remove a random key, any key# volatile-ttl -&gt; remove the key with the nearest expire time (minor TTL)# noeviction -&gt; don't expire at all, just return an error on write operations## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs a bit more CPU. 3 is very fast but not very accurate.## maxmemory-samples 5############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly yes# The name of the append only file (default: "appendonly.aof")appendfilename "7379.appendonly.aof"# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don't fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is "everysec", as that's usually the right compromise between# speed and data safety. It's up to you to understand if you can relax this to# "no" that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that's snapshotting),# or on the contrary, use "always" that's very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use "everysec".# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it's possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as "appendfsync none". In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to "yes". Otherwise leave it as# "no" that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can't happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the "redis-check-aof" utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn't want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however# in order to mark it as "mature" we need to wait for a non trivial percentage# of users to deploy it in production.# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++## Normal Redis instances can't be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A slave of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a slave to actually have a exact measure of# its "data age", so the following two checks are performed:## 1) If there are multiple slaves able to failover, they exchange messages# in order to try to give an advantage to the slave with the best# replication offset (more data from the master processed).# Slaves will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single slave computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the "connected" state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the slave will not try to failover# at all.## The point "2" can be tuned by user. Specifically a slave will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * slave-validity-factor) + repl-ping-slave-period## So for example if node-timeout is 30 seconds, and the slave-validity-factor# is 10, and assuming a default repl-ping-slave-period of 10 seconds, the# slave will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large slave-validity-factor may allow slaves with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a slave at all.## For maximum availability, it is possible to set the slave-validity-factor# to a value of 0, which means, that slaves will always try to failover the# master regardless of the last time they interacted with the master.# (However they'll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-slave-validity-factor 10# Cluster slaves are able to migrate to orphaned masters, that are masters# that are left without working slaves. This improves the cluster ability# to resist to failures as otherwise an orphaned master can't be failed over# in case of failure if it has no working slaves.## Slaves migrate to orphaned masters only if there are still at least a# given number of other working slaves for their old master. This number# is the "migration barrier". A migration barrier of 1 means that a slave# will migrate only if there is at least 1 other working slave for its master# and so forth. It usually reflects the number of slaves you want for every# master in your cluster.## Default is 1 (slaves migrate only if their masters remain with at least# one slave). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don't have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# "CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;" if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key "foo" stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# A Alias for g$lshzxe, so that the "AKE" string means all the events.## The "notify-keyspace-events" takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don't need# this feature and the feature has some overhead. Note that if you don't# specify at least one of K or E, no events will be delivered.notify-keyspace-events ""############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means "don't start compressing until after 1 node into the list,# going from either the head or tail"# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don't compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing "steps" are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use "activerehashing no" if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use "activerehashing yes" if you don't have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can't consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# slave -&gt; slave clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don't receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and slave clients, since# subscribers and slaves receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified "hz" value.## By default "hz" is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vagrant 虚拟机工具]]></title>
    <url>%2F2017%2F03%2F21%2F%E8%BF%90%E7%BB%B4%2Fvagrant%2F</url>
    <content type="text"><![CDATA[#vagrant [TOC] ##install 创建文件夹 vagrant init ubuntu/trusty64 vagrant up --provider virtualbox ##cmd |cmd|描述| |:| |vagrant status|状态 |vagrant halt|强制关闭，立刻释放所有的ram |vagrant suspend|挂起| |vagrant up|启动| |vagrant destroy|摧毁机器| |vagrant reload|重启机器| ##访问控制 12345678910# Create a forwarded port mapping which allows access to a specific port# within the machine from a port on the host machine. In the example below,# accessing &quot;localhost:8080&quot; will access port 80 on the guest machine.# config.vm.network &quot;forwarded_port&quot;, guest: 80, host: 8080Add the following directly below those comments: config.vm.network :forwarded_port, guest: 80, host: 8080Save the file and start your Vagrant virtual machine using the vagrant up command. If your virtual machine is currently running, you can reload it using the vagrant reload command.This configuration change will setup port forwarding from port 8080 on the host machine (your computer) to the guest machine (your Vagrant virtual machine) when your virtual machine is running. This will allow you to access your web server using the URL http://localhost:8080. ##smoe lesson 12345678910111213141516171819Vagrant CommandsWe’re now ready to get started working within our Linux virtual machine. If your download hasn’t completed from the initial setup, go ahead and take a break and come back when that has completed. You won’t be able to make further progress until the virtual machine is up and running as much of the course will take place within this environment.Before we access our machine, let’s quickly review a few commands that vagrant provides to make managing your virtual machines much simpler. Remember, your vagrant machine lives within this specific folder on your computer so make sure you’re within that same folder your created earlier; otherwise these commands won’t work as expected.Type vagrant statusThis command will show you the current status of the virtual machine. It should currently read “default running (virtualbox)” along with some other information.Type vagrant suspendThis command suspends your virtual machine. All of your work is saved and the machine is put into a “sleep mode” of sorts. The machines state is saved and it’s very quick to stop and start your work. You should use this command if you plan to just take a short break from your work but don’t want to leave the virtual machine running.Type vagrant upThis gets your virtual machine up and running again. Notice we didn’t have to redownload the virtual machine image, since it’s already been downloaded.Type vagrant sshThis command will actually connect to and log you into your virtual machine. Once done you will see a few lines of text showing various performance statistics of the virtual machine along with a new command line prompt that reads vagrant@vagrant-ubuntu-trusty-64:~$Here are a few other important commands that we’ll discuss but you do not need to practice at this time:vagrant haltThis command halts your virtual machine. All of your work is saved and the machine is turned off - think of this as “turning the power off”. It’s much slower to stop and start your virtual machine using this command, but it does free up all of your RAM once the machine has been stopped. You should use this command if you plan to take an extended break from your work, like when you are done for the day. The command vagrant up will turn your machine back on and you can continue your work.vagrant destroyThis command destroys your virtual machine. Your work is not saved, the machine is turned off and forgotten about for the most part. Think of this as formatting the hard drive of a computer. You can always use vagrant up to relaunch the machine but you’ll be left with the baseline Linux installation from the beginning of this course. You should not have to use this command at any time during this course unless, at some point in time, you perform a task on the virtual machine that makes it completely inoperable. 参考链接： first up帮助文档]]></content>
      <tags>
        <tag>linux 虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac vim 设置]]></title>
    <url>%2F2017%2F03%2F21%2F%E8%BF%90%E7%BB%B4%2Fvim%2F</url>
    <content type="text"><![CDATA[#mac vim 设置 vi ~/.vimrc 1234567891011121314set ai &quot; auto indentingset history=100 &quot; keep 100 lines of historyset ruler &quot; show the cursor positionsyntax on &quot; syntax highlightingset hlsearch &quot; highlight the last searched termfiletype plugin on &quot; use the file type plugins&quot; When editing a file, always jump to the last cursor positionautocmd BufReadPost *\ if ! exists(&quot;g:leave_my_cursor_position_alone&quot;) |\ if line(&quot;&apos;\&quot;&quot;) &gt; 0 &amp;&amp; line (&quot;&apos;\&quot;&quot;) &lt;= line(&quot;$&quot;) |\ exe &quot;normal g&apos;\&quot;&quot; |\ endif |\ endif]]></content>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux]]></title>
    <url>%2F2017%2F02%2F04%2F%E8%BF%90%E7%BB%B4%2Flinux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[#linux[TOC] ##系统 信息 命令 内核/操作系统/CPU信息 uname 操作系统版本 head -n 1 /etc/issue 计算机名 hostname 列出加载的内核模块 lsmod 查看环境变量 env 查看系统 ps -p 1 ##资源 信息 命令 查看内存使用量和交换区使用量 free -g 查看系统运行时间、用户数、负载 uptime 端口zan yong lsof -i grep 5601 查看硬盘 du -sh * `` ##用户 信息 命令 描述 新增 adduser 会自动为创建的用户指定主目录、系统shell版本，会在创建时输入用户密码。 新增 useradd 需要使用参数选项指定上述基本设置，如果不使用任何参数，则创建的用户无密码、无主目录、没有指定shell版本 添加root vi /etc/sudoers.d/$user &amp;&amp; $user ALL=(ALL) ALL``chmod $user_file 400 修改密码 sudo passwd $suser 修改用户权限 sudo chown -R elasticsearch:elasticsearch /data/local/elasticsearch ##进程|信息|命令|描述|:||kill|ps -ef | grep xmpp.php | grep -v grep | awk &#39;{print $2}&#39; | xargs kill| ##文件 信息 命令 描述 软链 ln -s /data/local/mysql5.6.16/bin/mysql /uer/bin 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960系统 # uname -a # 查看内核/操作系统/CPU信息 # head -n 1 /etc/issue # 查看操作系统版本 # cat /proc/cpuinfo # 查看CPU信息 # hostname # 查看计算机名 # lspci -tv # 列出所有PCI设备 # lsusb -tv # 列出所有USB设备 # lsmod # 列出加载的内核模块 # env # 查看环境变量资源 # free -m # 查看内存使用量和交换区使用量 # df -h # 查看各分区使用情况 # du -sh &lt;目录名&gt; # 查看指定目录的大小 # grep MemTotal /proc/meminfo # 查看内存总量 # grep MemFree /proc/meminfo # 查看空闲内存量 # uptime # 查看系统运行时间、用户数、负载 # cat /proc/loadavg # 查看系统负载磁盘和分区 # mount | column -t # 查看挂接的分区状态 # fdisk -l # 查看所有分区 # swapon -s # 查看所有交换分区 # hdparm -i /dev/hda # 查看磁盘参数（仅适用于IDE设备） # dmesg | grep IDE # 查看启动时IDE设备检测状况网络 # ifconfig # 查看所有网络接口的属性 # iptables -L # 查看防火墙设置 # route -n # 查看路由表 # netstat -lntp # 查看所有监听端口 # netstat -antp # 查看所有已经建立的连接 # netstat -s # 查看网络统计信息进程 # ps -ef # 查看所有进程 # top # 实时显示进程状态用户 # w # 查看活动用户 # id &lt;用户名&gt; # 查看指定用户信息 # last # 查看用户登录日志 # cut -d: -f1 /etc/passwd # 查看系统所有用户 # cut -d: -f1 /etc/group # 查看系统所有组 # crontab -l # 查看当前用户的计划任务服务 # chkconfig --list # 列出所有系统服务 # chkconfig --list | grep on # 列出所有启动的系统服务程序 # rpm -qa # 查看所有安装的软件包]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[auth_basic]]></title>
    <url>%2F2017%2F02%2F04%2Fnginx%2Fauth_basic%2F</url>
    <content type="text"><![CDATA[#nginx auth_basic 配置 nginx .conf 文件1234567891011121314151617upstream kibana5 &#123; server localhost:5601 fail_timeout=5; keepalive 64;&#125;server &#123; listen 5602; server_name kibana_server.com; access_log /data/log/nginx/kibana.srv-log-dev.log; error_log /data/log/nginx/kibana.srv-log-dev.error.log; location / &#123; auth_basic &quot;secret&quot;; auth_basic_user_file /data/local/nginx/passwd.db; proxy_pass http://localhost:5601; &#125;&#125; 配置的选项auth_basic &quot;secret&quot;; auth_basic_user_file /data/local/nginx/passwd.db; auth文件/data/local/nginx/passwd.db 我使用的是nginx服务器：为了方便直接使用在线工具生成的，格式如下：user:passwd]]></content>
      <tags>
        <tag>basrc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK 配置文件]]></title>
    <url>%2F2017%2F02%2F04%2Felk%2FELK-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[#ELK 配置文件[toc] ##配置文件 ###nginx123456789101112131415161718192021upstream kibana5 &#123; server localhost:5601 fail_timeout=5; keepalive 64;&#125;server &#123; listen 5602; server_name kibana_server.com; access_log /data/log/nginx/kibana.srv-log-dev.log; error_log /data/log/nginx/kibana.srv-log-dev.error.log;# ssl on;# ssl_certificate /etc/nginx/ssl/all.crt;# ssl_certificate_key /etc/nginx/ssl/server.key; location / &#123;# root /var/www/kibana;# index index.html index.htm; proxy_pass http://localhost:5601; &#125;&#125; ###文件地址 /etc/kibana/kibana.yml/data/local/elasticsearch/config/elasticsearch.yml/data/local/filebeat/filebeat.yml ###logstash.conf1234567891011121314151617181920input &#123; beats &#123; port =&gt; 5044 &#125;&#125;filter &#123; grok &#123; add_field =&gt; [ &quot;received_from&quot;, &quot;%&#123;from_machine&#125;&quot; ] &#125;&#125;output &#123; file &#123; path =&gt; &quot;/data/log/logstash/all.log&quot; codec =&gt; line &#123;format =&gt;&quot;$%&#123;from_machine&#125; %&#123;message&#125;&quot;&#125; flush_interval =&gt; 0 &#125;&#125; ###filebeat.yml(收集)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122###################### Filebeat Configuration Example ########################## This file is an example configuration file highlighting only the most common# options. The filebeat.full.yml file from the same directory contains all the# supported options with more comments. You can use it as a reference.## You can find the full configuration reference here:# https://www.elastic.co/guide/en/beats/filebeat/index.html#=========================== Filebeat prospectors =============================filebeat.prospectors:# Each - is a prospector. Most options can be set at the prospector level, so# you can use different prospectors for various configurations.# Below are the prospector specific configurations.- input_type: log # Paths that should be crawled and fetched. Glob based paths. # 收集的日志目录 paths: #- /var/log/*.log # #- c:\programdata\elasticsearch\logs\* - /data/log/api/api.log - /data/log/php/php.log # Exclude lines. A list of regular expressions to match. It drops the lines that are # matching any regular expression from the list. #exclude_lines: [&quot;^DBG&quot;] # Include lines. A list of regular expressions to match. It exports the lines that are # matching any regular expression from the list. #include_lines: [&quot;^ERR&quot;, &quot;^WARN&quot;] # Exclude files. A list of regular expressions to match. Filebeat drops the files that # are matching any regular expression from the list. By default, no files are dropped. #exclude_files: [&quot;.gz$&quot;] # Optional additional fields. These field can be freely picked # to add additional information to the crawled log files for filtering #fields: # level: debug # review: 1 ### Multiline options # Mutiline can be used for log messages spanning multiple lines. This is common # for Java Stack Traces or C-Line Continuation # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ #multiline.pattern: ^\[ # Defines if the pattern set under pattern should be negated or not. Default is false. #multiline.negate: false # Match can be set to &quot;after&quot; or &quot;before&quot;. It is used to define if lines should be append to a pattern # that was (not) matched before or after or as long as a pattern is not matched based on negate. # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash #multiline.match: after#================================ General =====================================# The name of the shipper that publishes the network data. It can be used to group# all the transactions sent by a single shipper in the web interface.#name:# The tags of the shipper are included in their own field with each# transaction published.#tags: [&quot;service-X&quot;, &quot;web-tier&quot;]# Optional fields that you can specify to add additional information to the# output.#fields:# env: staging#================================ Outputs =====================================# Configure what outputs to use when sending the data collected by the beat.# Multiple outputs may be used.#-------------------------- Elasticsearch output ------------------------------#output.elasticsearch: # Array of hosts to connect to.# hosts: [&quot;localhost:9200&quot;] # Optional protocol and basic auth credentials. #protocol: &quot;https&quot; #username: &quot;elastic&quot; #password: &quot;changeme&quot;#----------------------------- Logstash output --------------------------------#fields_under_root: truefields: from_machine: &quot;10.8.15.106 APP[5]&quot;output.logstash: # The Logstash hosts hosts: [&quot;10.8.26.121:5044&quot;] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [&quot;/etc/pki/root/ca.pem&quot;] # Certificate for SSL client authentication #ssl.certificate: &quot;/etc/pki/client/cert.pem&quot; # Client Certificate Key #ssl.key: &quot;/etc/pki/client/cert.key&quot;#================================ Logging =====================================# Sets log level. The default log level is info.# Available log levels are: critical, error, warning, info, debug#logging.level: debug# At debug level, you can selectively enable logging only for some components.# To enable all selectors use [&quot;*&quot;]. Examples of other selectors are &quot;beat&quot;,# &quot;publish&quot;, &quot;service&quot;.#logging.selectors: [&quot;*&quot;] ###filebeat.yml (加密) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119###################### Filebeat Configuration Example ########################## This file is an example configuration file highlighting only the most common# options. The filebeat.full.yml file from the same directory contains all the# supported options with more comments. You can use it as a reference.## You can find the full configuration reference here:# https://www.elastic.co/guide/en/beats/filebeat/index.html#=========================== Filebeat prospectors =============================filebeat.prospectors:# Each - is a prospector. Most options can be set at the prospector level, so# you can use different prospectors for various configurations.# Below are the prospector specific configurations.- input_type: log # Paths that should be crawled and fetched. Glob based paths. paths: - /data/log/lang-xmpp/api.log #- /var/log/*.log #- c:\programdata\elasticsearch\logs\* # Exclude lines. A list of regular expressions to match. It drops the lines that are # matching any regular expression from the list. #exclude_lines: [&quot;^DBG&quot;] # Include lines. A list of regular expressions to match. It exports the lines that are # matching any regular expression from the list. #include_lines: [&quot;^ERR&quot;, &quot;^WARN&quot;] # Exclude files. A list of regular expressions to match. Filebeat drops the files that # are matching any regular expression from the list. By default, no files are dropped. #exclude_files: [&quot;.gz$&quot;] # Optional additional fields. These field can be freely picked # to add additional information to the crawled log files for filtering #fields: # level: debug # review: 1 ### Multiline options # Mutiline can be used for log messages spanning multiple lines. This is common # for Java Stack Traces or C-Line Continuation # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ #multiline.pattern: ^\[ # Defines if the pattern set under pattern should be negated or not. Default is false. #multiline.negate: false # Match can be set to &quot;after&quot; or &quot;before&quot;. It is used to define if lines should be append to a pattern # that was (not) matched before or after or as long as a pattern is not matched based on negate. # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash #multiline.match: after#================================ General =====================================fields_under_root: true#fields:# from_machine: &quot;10.8.17.112 APP[1]&quot;# The name of the shipper that publishes the network data. It can be used to group# all the transactions sent by a single shipper in the web interface.#name:# The tags of the shipper are included in their own field with each# transaction published.#tags: [&quot;service-X&quot;, &quot;web-tier&quot;]# Optional fields that you can specify to add additional information to the# output.#fields:# env: staging#================================ Outputs =====================================# Configure what outputs to use when sending the data collected by the beat.# Multiple outputs may be used.#-------------------------- Elasticsearch output ------------------------------output.elasticsearch: # Array of hosts to connect to. hosts: [&quot;10.8.17.112:9200&quot;] index: &quot;xmpp-%&#123;+yyyy.MM.dd&#125;&quot; # Optional protocol and basic auth credentials. #protocol: &quot;https&quot; username: &quot;elastic&quot; password: &quot;elasticpassword0314&quot;#----------------------------- Logstash output --------------------------------#output.logstash:# hosts: [&quot;10.8.26.121:5044&quot;] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [&quot;/etc/pki/root/ca.pem&quot;] # Certificate for SSL client authentication #ssl.certificate: &quot;/etc/pki/client/cert.pem&quot; # Client Certificate Key #ssl.key: &quot;/etc/pki/client/cert.key&quot;#================================ Logging =====================================# Sets log level. The default log level is info.# Available log levels are: critical, error, warning, info, debug#logging.level: debug# At debug level, you can selectively enable logging only for some components.# To enable all selectors use [&quot;*&quot;]. Examples of other selectors are &quot;beat&quot;,# &quot;publish&quot;, &quot;service&quot;.#logging.selectors: [&quot;*&quot;]~ ~ ###elasticsearch1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.# Before you set out to tweak and tune the configuration, make sure you# understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:##cluster.name: my-application## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/data## Path to log files:##path.logs: /path/to/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: truebootstrap.memory_lock: falsebootstrap.system_call_filter: false## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):#network.host: 10.8.17.112## Set a custom port for HTTP:#http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]##discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]## Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: 3## For more information, consult the zen discovery module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: trueaction.auto_create_index: .security,.monitoring*,.watches,.triggered_watches,.watcher-history*xpack.security.audit.enabled: true ###kibana.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102Kibana is served by a back end server. This setting specifies the port to use.server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is &apos;localhost&apos;, which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.server.host: &quot;localhost&quot;# Enables you to specify a path to mount Kibana at if you are running behind a proxy. This only affects# the URLs generated by Kibana, your proxy is expected to remove the basePath value before forwarding requests# to Kibana. This setting cannot end in a slash.#server.basePath: &quot;&quot;# The maximum payload size in bytes for incoming server requests.#server.maxPayloadBytes: 1048576# The Kibana server&apos;s name. This is used for display purposes.#server.name: &quot;your-hostname&quot;# The URL of the Elasticsearch instance to use for all your queries.elasticsearch.url: &quot;http://10.8.17.112:9200&quot;# When this setting&apos;s value is true Kibana uses the hostname specified in the server.host# setting. When the value of this setting is false, Kibana uses the hostname of the host# that connects to this Kibana instance.#elasticsearch.preserveHost: true# Kibana uses an index in Elasticsearch to store saved searches, visualizations and# dashboards. Kibana creates a new index if the index doesn&apos;t already exist.kibana.index: &quot;.kibana&quot;# The default application to load.#kibana.defaultAppId: &quot;discover&quot;# If your Elasticsearch is protected with basic authentication, these settings provide# the username and password that the Kibana server uses to perform maintenance on the Kibana# index at startup. Your Kibana users still need to authenticate with Elasticsearch, which# is proxied through the Kibana server.elasticsearch.username: &quot;kibana&quot;elasticsearch.password: &quot;kibanapassword0314&quot;# Paths to the PEM-format SSL certificate and SSL key files, respectively. These# files enable SSL for outgoing requests from the Kibana server to the browser.#server.ssl.cert: /path/to/your/server.crt#server.ssl.key: /path/to/your/server.key# Optional settings that provide the paths to the PEM-format SSL certificate and key files.# These files validate that your Elasticsearch backend uses the same key files.#elasticsearch.ssl.cert: /path/to/your/client.crt#elasticsearch.ssl.key: /path/to/your/client.key# Optional setting that enables you to specify a path to the PEM file for the certificate# authority for your Elasticsearch instance.#elasticsearch.ssl.ca: /path/to/your/CA.pem# To disregard the validity of SSL certificates, change this setting&apos;s value to false.#elasticsearch.ssl.verify: true# Time in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of# the elasticsearch.requestTimeout setting.#elasticsearch.pingTimeout: 1500# Time in milliseconds to wait for responses from the back end or Elasticsearch. This value# must be a positive integer.#elasticsearch.requestTimeout: 30000# List of Kibana client-side headers to send to Elasticsearch. To send *no* client-side# headers, set this value to [] (an empty list).#elasticsearch.requestHeadersWhitelist: [ authorization ]# Header names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten# by client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration.#elasticsearch.customHeaders: &#123;&#125;# Time in milliseconds for Elasticsearch to wait for responses from shards. Set to 0 to disable.#elasticsearch.shardTimeout: 0# Time in milliseconds to wait for Elasticsearch at Kibana startup before retrying.#elasticsearch.startupTimeout: 5000# Specifies the path where Kibana creates the process ID file.#pid.file: /var/run/kibana.pid# Enables you specify a file where Kibana stores log output.#logging.dest: stdout# Set the value of this setting to true to suppress all logging output.#logging.silent: false# Set the value of this setting to true to suppress all logging output other than error messages.#logging.quiet: false# Set the value of this setting to true to log all events, including system usage information# and all requests.#logging.verbose: false# Set the interval in milliseconds to sample system and process performance# metrics. Minimum is 100ms. Defaults to 5000.#ops.interval: 5000#xpack.security.enabled: true]]></content>
  </entry>
  <entry>
    <title><![CDATA[异常处理（持续更新...）]]></title>
    <url>%2F2017%2F02%2F04%2Fnginx%2Fnginx500%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[#nginx 500 解决方案汇总 [toc] ##检查php-fpm 进程 原因：脚本执行时间超时，导致php-fpm进程被占用方案：临时：添加php-fpm进程数步骤：1，ps -ef | grep php-fpm 查看当前php-fpm的配置文件2，pm = static &amp;&amp; pm.max_children = 128]]></content>
      <tags>
        <tag>nginx 服务端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK 搭建文档]]></title>
    <url>%2F2017%2F02%2F04%2Felk%2FELK-%E6%90%AD%E5%BB%BA%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[#ELK 搭建[TOC] ⚠️ 请保持全程elk各系统间的版本一致elk的数据增长很快，请在开始的时候注意切分好index，方便清理旧数据本文档仅适用于参考，不同的服务器，版本等信息均需要参看官方教程一切以官网文档为第一手资料 安装：filebean下载1curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-5.1.1-x86_64.rpm 安装1rpm -vi filebeat-5.1.1-x86_64.rpm 配置1vi /etc/filebeat/filebeat.yml ####软链1ln -s /etc/filebeat /data/local/filebeat logstash:java环境依赖12yum install java-1.8.0-openjdkyum install java-1.8.0-openjdk-devel.x86_64 export JAVACMD=`which java` export JAVA_HOME=`which java` 下载123curl -L -O https://artifacts.elastic.co/downloads/logstash/logstash-5.1.1.rpmsudo rpm -i logstash-5.1.1.rpm 软链：1ln -s /etc/logstash /data/local/logstash warning: logstash-5.1.1.rpm: Header V4 RSA/SHA512 Signature, key ID d88e42b4: NOKEY 插件： plugin 接收插件，gem_source 墙 /usr/share/logstash/Gemfile filebeat1/usr/share/logstash/bin/logstash-plugin install logstash-input-beats start bug fix/usr/bin/filebeat.sh -e -c /data/local/filebeat/filebeat.yml 启动filebean测试1/usr/share/filebeat/bin/filebeat -e -c filebeat.yml -d "publish" 1/usr/share/filebeat/bin/filebeat -e -c /data/local/filebeat/filebeat.yml -d &quot;publish&quot; 生产1nohup /usr/bin/filebeat.sh -e -c /data/local/filebeat/filebeat.yml &amp;&gt;/dev/null &amp; 清空 测试数据复用1rm /usr/share/filebeat/bin/data/registry logstash测试1234/usr/share/logstash/bin/logstash -f /data/local/logstash/conf.d/logstash.conf/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash.confbin/logstash -f first-pipeline.conf --config.test_and_exitbin/logstash -f first-pipeline.conf --config.reload.automatic 生产@(工具)1nohup /usr/share/logstash/bin/logstash -f /data/local/logstash/conf.d/logstash.conf &gt;&gt; /data/ elasticsearch:概览： Check your cluster, node, and index health, status, and statistics Administer your cluster, node, and index data and metadata Perform CRUD (Create, Read, Update, and Delete) and search operations against your indexes Execute advanced search operations such as paging, sorting, filtering, scripting, aggregations, and many others 下载1234 curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.1.1.tar.gztar -xvf elasticsearch-5.1.1.tar.gzcd elasticsearch-5.1.1/bin 启动12./elasticsearch./elasticsearch -Ecluster.name=my_cluster_name -Enode.name=my_node_name 测试helath check1curl -XGET 'localhost:9200/_cat/health?v&amp;pretty' list of node1curl -XGET 'localhost:9200/_cat/nodes?v&amp;pretty' index of node :1curl -XGET 'localhost:9200/_cat/indices?v&amp;pretty' create index ：1curl -XPUT 'localhost:9200/customer?pretty&amp;pretty' 让我们将一个简单的客户文档索引到客户索引“外部”类型，ID为1，如下所示： 新增=修改1234curl -XPUT 'localhost:9200/customer/external/1?pretty&amp;pretty' -d'&#123; "name": "John Doe"&#125;' 查看12curl -XGET 'localhost:9200/customer/external/1?pretty&amp;pretty'curl -XGET 'localhost:9200/customer/external/1?pretty' 删除1curl -XDELETE 'localhost:9200/customer?pretty' del index1curl -XDELETE 'localhost:9200/customer?pretty&amp;pretty' list index1curl -XGET 'localhost:9200/_cat/indices?v&amp;pretty' elastic_search 本轮存在很大问题啊1234567curl -XPUT 'localhost:9200/customer?pretty'curl -XPUT 'localhost:9200/customer/external/2?pretty' -d'&#123; "name": "John"&#125;'curl -XGET 'localhost:9200/customer/external/1?pretty'curl -XDELETE 'localhost:9200/customer?pretty' that in the above case, we are using the POST verb instead of PUT since we didn’t specify an ID. 自增1234curl -XPOST 'localhost:9200/customer/external?pretty&amp;pretty' -d'&#123; "name": "Jane Doe"&#125;' 修改11234curl -XPOST 'localhost:9200/customer/external/1/_update?pretty&amp;pretty' -d'&#123; "doc": &#123; "name": "Jane Doe", "age": 20 &#125;&#125;' 修改21234curl -XPOST 'localhost:9200/customer/external/1/_update?pretty&amp;pretty' -d'&#123; "script" : "ctx._source.age += 5"&#125;' 修改 put可以直接进行创建的时候修改1curl -XPOST 'localhost:9200/customer/external/1/_update?pretty&amp;pretty' -d' ####批量操作目前测试不通过，只有批量的第一个会成功12345curl -XPOST 'localhost:9200/customer/external/_bulk?pretty&amp;pretty' -d'&#123;"index":&#123;"_id":"1"&#125;&#125;&#123;"name": "John Doe111" &#125;&#123;"index":&#123;"_id":"2"&#125;&#125;&#123;"name": "Jane Doe222" &#125;' 实际数据操作准备数据123wget https://raw.githubusercontent.com/elastic/elasticsearch/master/docs/src/test/resources/accounts.jsoncurl -XPOST 'localhost:9200/bank/account/_bulk?pretty&amp;refresh' --data-binary "@accounts.json"curl 'localhost:9200/_cat/indices?v' 查询 REST request URI 1curl -XGET 'localhost:9200/bank/_search?q=*&amp;sort=account_number:asc&amp;pretty&amp;pretty' 查询 REST request body 1234567curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": [ &#123; "account_number": "asc" &#125; ]&#125;' 查询扩展 REST request body size 默认 10123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198查询所有curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125;'查询所有 返回1curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "size": 1&#125;'查询所有 返回10条curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "from": 10, "size": 10&#125;'查询所有 返回字段仅 account_number balancecurl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "_source": ["account_number", "balance"]&#125;'查询匹配 account_number = 20curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "match": &#123; "account_number": 20 &#125; &#125;&#125;'查询匹配 curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "match": &#123; "address": "mill" &#125; &#125;&#125;'查询匹配 orcurl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "match": &#123; "address": "mill lane" &#125; &#125;&#125;'查询匹配 &amp;&amp;curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "match_phrase": &#123; "address": "mill lane" &#125; &#125;&#125;'查询匹配 &amp;&amp;curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;'查询 ！curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "bool": &#123; "must_not": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;'组合curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "age": "40" &#125; &#125; ], "must_not": [ &#123; "match": &#123; "state": "ID" &#125; &#125; ] &#125; &#125;&#125;'范围curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "query": &#123; "bool": &#123; "must": &#123; "match_all": &#123;&#125; &#125;, "filter": &#123; "range": &#123; "balance": &#123; "gte": 20000, "lte": 30000 &#125; &#125; &#125; &#125; &#125;&#125;'分组查询curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state.keyword" &#125; &#125; &#125;&#125;'＝＝SELECT state, COUNT(*) FROM bank GROUP BY state ORDER BY COUNT(*) DESCcurl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state.keyword", "order": &#123; "average_balance": "desc" &#125; &#125;, "aggs": &#123; "average_balance": &#123; "avg": &#123; "field": "balance" &#125; &#125; &#125; &#125; &#125;&#125;'curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state.keyword" &#125;, "aggs": &#123; "average_balance": &#123; "avg": &#123; "field": "balance" &#125; &#125; &#125; &#125; &#125;&#125;'这个例子演示了我们如何根据年龄段（20-29，30-39和40-49），然后按性别分组，然后最终得到每个年龄段的每个性别的平均帐户余额！！！curl -XGET 'localhost:9200/bank/_search?pretty' -d'&#123; "size": 0, "aggs": &#123; "group_by_age": &#123; "range": &#123; "field": "age", "ranges": [ &#123; "from": 20, "to": 30 &#125;, &#123; "from": 30, "to": 40 &#125;, &#123; "from": 40, "to": 50 &#125; ] &#125;, "aggs": &#123; "group_by_gender": &#123; "terms": &#123; "field": "gender.keyword" &#125;, "aggs": &#123; "average_balance": &#123; "avg": &#123; "field": "balance" &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;' elastic search 扩展练习1234curl -XGET &apos;127.0.0.1:9200/xmpp/_search?pretty&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;message&quot;: &quot;1000010=&gt;1279133&quot; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113GET /megacorp/employee/_search?prettyGET /megacorp/employee/_search?q=last_name:SmithGET /megacorp/employee/_search?pretty&#123; "query" : &#123; "match" : &#123; "last_name" : "Smith" &#125; &#125;&#125;GET /megacorp/employee/_search?pretty&#123; "query" : &#123; "bool" : &#123; "must" : &#123; "match" : &#123; "last_name" : "smith" &#125; &#125;, "filter" : &#123; "range" : &#123; "age" : &#123; "gt" : 30 &#125; &#125; &#125; &#125; &#125;&#125;GET /megacorp/employee/_search&#123; "query" : &#123; "match" : &#123; "about" : "rock climbing" &#125; &#125;&#125;GET /megacorp/employee/_search&#123; "query" : &#123; "match_phrase" : &#123; "about" : "rock climbing" &#125; &#125;&#125;GET /megacorp/employee/_search&#123; "query" : &#123; "match_phrase" : &#123; "about" : "rock climbing" &#125; &#125;, "highlight": &#123; "fields" : &#123; "about" : &#123;&#125; &#125; &#125;&#125;GET /megacorp/employee/_search&#123; "aggs": &#123; "all_interests": &#123; "terms": &#123; "field": "interests" &#125; &#125; &#125;&#125;PUT /megacorp/_mapping/employee?pretty&#123; "properties": &#123; "interests": &#123; "type": "text", "fielddata": true &#125; &#125;&#125;GET /megacorp/employee/_search&#123; "query": &#123; "match": &#123; "last_name": "smith" &#125; &#125;, "aggs": &#123; "all_interests": &#123; "terms": &#123; "field": "interests" &#125; &#125; &#125;&#125;GET /megacorp/employee/_search&#123; "aggs" : &#123; "all_interests" : &#123; "terms" : &#123; "field" : "interests" &#125;, "aggs" : &#123; "avg_age" : &#123; "avg" : &#123; "field" : "age" &#125; &#125; &#125; &#125; &#125;&#125; kibana安装vi /etc/yum.repos.d/kibana.repo123456789[kibana-5.x]name=Kibana repository for 5.x packagesbaseurl=https://artifacts.elastic.co/packages/5.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=1autorefresh=1type=rpm-md 12345sudo yum install kibanasudo chkconfig --add kibanasudo -i service kibana startsudo -i service kibana stop 运行sudo -i service kibana start 配置nginx转发1sudo -i service kibana stop elastic search生产环境部署bug fix：Q1:ERROR: bootstrap checks failed system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk1234567原因：这是在因为Centos6不支持SecComp，而ES5.2.0默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动。解决：在elasticsearch.yml中配置bootstrap.system_call_filter为false，注意要在Memory下面:bootstrap.memory_lock: falsebootstrap.system_call_filter: false` Q2:max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]12345678910111213`ulimit -n 5000`解决：切换到root用户，进入limits.d目录下修改配置文件。vi /etc/security/limits.d/90-nproc.conf 修改如下内容：* soft nproc 1024#修改为* soft nproc 2048 Q3max number of threads [1024] for user [lishang] likely too low, increase to at least [2048]12345678910111213解决：切换到root用户修改配置sysctl.confvi /etc/sysctl.conf 添加下面配置：vm.max_map_count=655360并执行命令：sysctl -p然后，重新启动elasticsearch，即可启动成功。 x-pack x-pack 是需要license的，请安装的时候注意，如果不准备申请license的话：配置如下：elasticsearch.yml1234xpack.security.enabled: falsexpack.monitoring.enabled: truexpack.graph.enabled: false#xpack.reporting.enabled: false 123some warm：Storing generated key in [/Users/langlive/Desktop/elasticsearch-5.2.1/config/x-pack/system_key]...Ensure the generated key can be read by the user that Elasticsearch runs as, permissions are set to owner read/write only 安装elasticsearch1bin/elasticsearch-plugin install x-pack kibana1bin/kibana-plugin install x-pack logstash1bin/logstash-plugin install x-pack filebeat1vim /data/local/filebeat/filebeat.yml 设置密码1234567891011curl -XPUT -u elastic &apos;127.0.0.1:9200/_xpack/security/user/elastic/_password&apos; -d &apos;&#123; &quot;password&quot; : &quot;123456&quot;&#125;&apos;curl -XPUT -u elastic &apos;127.0.0.1:9200/_xpack/security/user/kibana/_password&apos; -d &apos;&#123; &quot;password&quot; : &quot;123456&quot;&#125;&apos;curl -XPUT -u elastic &apos;127.0.0.1:9200/_xpack/security/user/logstash_system/_password&apos; -d &apos;&#123; &quot;password&quot; : &quot;123456&quot;&#125;&apos; 访问控制创建角色1234567891011121314151617181920212223242526272829303132curl -XPOST -u elastic &apos;127.0.0.1:9200/_xpack/security/role/events_admin&apos; -d &apos;&#123; &quot;indices&quot; : [ &#123; &quot;names&quot; : [ &quot;events*&quot; ], &quot;privileges&quot; : [ &quot;all&quot; ] &#125;, &#123; &quot;names&quot; : [ &quot;xmpp*&quot; ], &quot;privileges&quot; : [ &quot;all&quot; ] &#125;, &#123; &quot;names&quot; : [ &quot;log*&quot; ], &quot;privileges&quot; : [ &quot;all&quot; ] &#125;, &#123; &quot;names&quot; : [ &quot;.kibana*&quot; ], &quot;privileges&quot; : [ &quot;manage&quot;, &quot;read&quot;, &quot;index&quot; ] &#125; ]&#125;&apos;curl -XPOST -u elastic &apos;127.0.0.1:9200/_xpack/security/role/events_root&apos; -d &apos;&#123; &quot;cluster&quot;:all &quot;indices&quot; : [ &#123; &quot;names&quot; : [ &quot;*&quot; ], &quot;privileges&quot; : [ &quot;all&quot; ] &#125;, ]&#125;&apos; 创建管理员1234567891011121314curl -XPOST -u elastic &apos;127.0.0.1:9200/_xpack/security/user/walter&apos; -d &apos;&#123; &quot;password&quot; : &quot;enjoyprocess&quot;, &quot;full_name&quot; : &quot;walter.shi&quot;, &quot;email&quot; : &quot;walter.shi@langlive.com&quot;, &quot;roles&quot; : [ &quot;events_admin&quot; ]&#125;&apos;curl -XPOST -u elastic &apos;127.0.0.1:9200/_xpack/security/user/dev&apos; -d &apos;&#123; &quot;password&quot; : &quot;kibana-search&quot;, &quot;full_name&quot; : &quot;langlive.dev&quot;, &quot;email&quot; : &quot;dev@langlive.com&quot;, &quot;roles&quot; : [ &quot;events_admin&quot; ]&#125;&apos; 修改密码123456curl -XPUT &apos;localhost:9200/_xpack/security/user/elastic/_password?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;password&quot;: &quot;elasticpassword&quot;&#125;&apos; 参考链接：filebeat安装文档教程filebeat配置文档教程logstash安装文档教程logstash安装插件教程logstash配置文档教程]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F01%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>